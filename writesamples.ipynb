{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Tue Feb  4 10:05:49 2020\n",
    "@author: kelleclark\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import numpy\n",
    "import matplotlib\n",
    "import sys\n",
    "import nltk\n",
    "#nltk.download('gutenberg')\n",
    "#nltk.download('genesis')\n",
    "#nltk.download('inaugural')\n",
    "#nltk.download('nps_chat')\n",
    "#nltk.download('webtext')\n",
    "#nltk.download('treebank')\n",
    "#from nltk.data import *\n",
    "#from nltk.book import *\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    " \n",
    "#from keras.models import Sequential\n",
    "#from keras.layers import Dense, Dropout, LSTM\n",
    "#from keras.utils import np_utils\n",
    "#from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# enumerate ngrams from Eisentein and CSCI6040 ipynb\n",
    "def ngrams(instring, n):\n",
    "    outset = {}\n",
    "    for i in range(len(instring) - n + 1):\n",
    "        g = ' '.join(instring[i:i+n])\n",
    "        outset.setdefault(g, 0)\n",
    "        outset[g] += 1\n",
    "    return outset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngramModel(instring, n):\n",
    "   outset = tokenize_words(instring)\n",
    "   totalpossible = len(outset)\n",
    "   #print (outset)\n",
    "   anoutcome = ngrams(outset,n)\n",
    "   if(n != 1):\n",
    "     previousoutcome = ngrams(outset,n-1)\n",
    "   #print(anoutcome)\n",
    "   probmodel = anoutcome\n",
    "   grams = anoutcome\n",
    "   test = 0\n",
    "   for keyword in anoutcome:\n",
    "        print(f\"the word is {keyword} and it occurs {probmodel[keyword]} times\")\n",
    "        if(n==1):\n",
    "          probmodel[keyword] = (probmodel[keyword] / totalpossible)\n",
    "          test = test + probmodel[keyword]\n",
    "        else: \n",
    "          listword = keyword.split()\n",
    "          print(listword[0])\n",
    "          prob1 = previousoutcome[listword[0]] / totalpossible\n",
    "          probmodel[keyword] = prob1 *(probmodel[keyword] / previousoutcome[listword[0]])\n",
    "          test = test + probmodel[keyword]\n",
    "   print(probmodel)\n",
    "   print(f\"The sum of all he probabiities need to be 1 and it is {test}\")\n",
    "   return probmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneCorpusFromFiles(folderpath):\n",
    "    textfiles = [f for f in os.listdir(folderpath) if '.txt' in f]\n",
    "    corpus = ' '\n",
    "    for f in textfiles:\n",
    "        try:\n",
    "            substring = ' '\n",
    "            with open(folderpath + '/'+ f) as filetext:\n",
    "                print(f\"the language of file \"+f+\" is {nltk.language(filetext)}\")\n",
    "                substring = substring.join(line.strip() for line in filetext)\n",
    "                filetext.close()\n",
    "                corpus = corpus + substring\n",
    "        except:\n",
    "            print(\"Error reading file:\" + f)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_words(instring):\n",
    "    # lowercase and remove punctuation to standardize it\n",
    "    tokens = word_tokenize(instring)\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "    #creating a regular expression for matching\n",
    "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    punct_free_words = [re_punc.sub('', w) for w in tokens]\n",
    "    words = [word for word in punct_free_words if word.isalpha()]\n",
    "    print(f\"\\n NLTK identified tokenized {len(words)} words.\")\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printoutcorpus(instring,folderpath):\n",
    "    try:\n",
    "        if len(instring) > 500: \n",
    "            print(folderpath + \"\\n   First & last 50 characters of Corpus:\\n\" + instring[:50]+ \" . . . \"+ instring[-50:])\n",
    "        elif (len(instring) > 0):\n",
    "            print(folderpath+f\"\\n {instring[:]}\")\n",
    "        else:\n",
    "            print(folderpath + \"\\n   Corpus is under 100 characters:\\n'{corpus}'\")\n",
    "    except:\n",
    "        print(\"The corpus does not exist.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def ngramModel_Smooth(instring, n):\n",
    "    #generate ngrams\n",
    "    outset = tokenize_words(instring)\n",
    "    total = len(outset)\n",
    "    anoutcome = []\n",
    "    for i in range(1,n+1):\n",
    "        print(i)\n",
    "        anoutcome.append(ngrams(outset, i))\n",
    "        print(\"outcome: \")\n",
    "        print(anoutcome[i-1])\n",
    "    #generate lamd\n",
    "    k = 1\n",
    "    lamd = []\n",
    "    last_lamd = 0\n",
    "    for i in range(1,n):\n",
    "        lamd.append(random.uniform(0,k))\n",
    "        k = k-lamd[i -1]              \n",
    "    lamd.append(k)\n",
    "    print(\"lamd: \", lamd)\n",
    "    #generate smooth model\n",
    "    smooth_model = {}\n",
    "    for keyword in anoutcome[n-1]:\n",
    "        grams = keyword.split(' ')\n",
    "        #print(\"grams:\")\n",
    "        #print(grams)\n",
    "        smooth_model.setdefault(keyword, lamd[0]*anoutcome[0][grams[0]]/total)\n",
    "        for i in range(1,len(grams) - 2):\n",
    "            sub_string = ' '.join(grams[0:i])\n",
    "            sub_sub_string = ' '.join(input[0:i -1])\n",
    "            print(sub_string)\n",
    "            smooth_model[keyword] = smooth_model[keyword] + lamd[i] * (anoutcome[i][sub_string]/anoutcome[i-1][keyword])\n",
    "        print(keyword + \":\")\n",
    "        print(smooth_model[keyword])\n",
    "    print(\"smooth_model:\")\n",
    "    print(smooth_model)\n",
    "    return smooth_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " NLTK identified tokenized 9 words.\n",
      "1\n",
      "outcome: \n",
      "{'this': 2, 'is': 2, 'short': 1, 'test': 1, 'a': 1, 'new': 1, 'doc': 1}\n",
      "2\n",
      "outcome: \n",
      "{'this is': 2, 'is short': 1, 'short test': 1, 'test this': 1, 'is a': 1, 'a new': 1, 'new doc': 1}\n",
      "3\n",
      "outcome: \n",
      "{'this is short': 1, 'is short test': 1, 'short test this': 1, 'test this is': 1, 'this is a': 1, 'is a new': 1, 'a new doc': 1}\n",
      "lamd:  [0.8323338367213761, 0.06550511845913445, 0.10216104481948948]\n",
      "this is short:\n",
      "0.18496307482697247\n",
      "is short test:\n",
      "0.18496307482697247\n",
      "short test this:\n",
      "0.09248153741348623\n",
      "test this is:\n",
      "0.09248153741348623\n",
      "this is a:\n",
      "0.18496307482697247\n",
      "is a new:\n",
      "0.18496307482697247\n",
      "a new doc:\n",
      "0.09248153741348623\n",
      "smooth_model:\n",
      "{'this is short': 0.18496307482697247, 'is short test': 0.18496307482697247, 'short test this': 0.09248153741348623, 'test this is': 0.09248153741348623, 'this is a': 0.18496307482697247, 'is a new': 0.18496307482697247, 'a new doc': 0.09248153741348623}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'this is short': 0.18496307482697247,\n",
       " 'is short test': 0.18496307482697247,\n",
       " 'short test this': 0.09248153741348623,\n",
       " 'test this is': 0.09248153741348623,\n",
       " 'this is a': 0.18496307482697247,\n",
       " 'is a new': 0.18496307482697247,\n",
       " 'a new doc': 0.09248153741348623}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngramModel_Smooth(\"This is short test, This is a new doc\", 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Generate_text_with_ngrams_model(seedtext):\n",
    "    pathname = 'Test Data/short test data'\n",
    "    corp = oneCorpusFromFiles(pathname)\n",
    "    tokenedcorp = tokenize_words(corp)\n",
    "    unigrams = ngrams(tokenedcorp,1)\n",
    "    bigrams = ngrams(tokenedcorp,2)\n",
    "    trigrams = ngrams(tokenedcorp,3)\n",
    "    quadgrams = ngrams(tokenedcorp,4)\n",
    "    r = 10#random.randrange(5,100)\n",
    "    print(r)\n",
    "    #using bigram generate r lengh text\n",
    "    for char in range(0,len(seedtext)):\n",
    "        maybe = seedtext[:len(seedtext)-char]\n",
    "        print(f\"Searching unigrams for '{maybe}'.\") \n",
    "        if maybe in unigrams:\n",
    "            currentword = maybe\n",
    "            break\n",
    "        print(\"Not found. Dropping a letter\")\n",
    "        currentword = \"diffenent\" \n",
    "    start_text = currentword \n",
    "    print(\"Start generate\")    \n",
    "    output_bigrams_model = ''\n",
    "    output_bigrams_model = output_bigrams_model.join(currentword)\n",
    "    add_text = \"\"\n",
    "    for i in range(1,r):\n",
    "        high_prob = 0\n",
    "        sub_grams = dict((key,bigrams[key]) for key in bigrams if key.startswith(currentword + \" \"))\n",
    "        for keyword in sub_grams:            \n",
    "            if sub_grams[keyword] > high_prob:\n",
    "                add_text = keyword[len(currentword)::]\n",
    "                high_prob = bigrams[keyword]\n",
    "        output_bigrams_model = output_bigrams_model + add_text\n",
    "        currentword = add_text.strip()        \n",
    "    print(\"Generate text by bigrams model:\")\n",
    "    print(output_bigrams_model)\n",
    "    #Generate text by trigrams model\n",
    "    currentword = start_text\n",
    "    sub_grams = dict((key,bigrams[key]) for key in bigrams if key.startswith(currentword + \" \"))\n",
    "    add_text = \"\"\n",
    "    output_trigrams_model = ''\n",
    "    output_trigrams_model = output_trigrams_model.join(currentword)\n",
    "    high_prob = 0\n",
    "    for keyword in sub_grams:\n",
    "        if sub_grams[keyword] > high_prob:\n",
    "                add_text = keyword[len(currentword)::]\n",
    "                high_prob = bigrams[keyword]\n",
    "    output_trigrams_model = output_trigrams_model + add_text\n",
    "    currentword = output_trigrams_model;\n",
    "    for i in range(1,r -1):\n",
    "        high_prob = 0\n",
    "        sub_grams = dict((key,trigrams[key]) for key in trigrams if key.startswith(currentword + \" \"))\n",
    "        for keyword in sub_grams:            \n",
    "            if sub_grams[keyword] > high_prob:\n",
    "                add_text = keyword[len(currentword)::]\n",
    "                high_prob = trigrams[keyword]\n",
    "        print(\"add_text\")\n",
    "        print(add_text)\n",
    "        output_trigrams_model = output_trigrams_model + add_text\n",
    "        currentword = currentword.split(\" \")[1] + \" \" + add_text.strip()    \n",
    "    print(\"Generate text by trigrams model:\")\n",
    "    print(output_trigrams_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the language of file tragedy-of-macbeth.txt is {nltk.language(filetext)}\n",
      "\n",
      " NLTK identified tokenized 19803 words.\n",
      "10\n",
      "Searching unigrams for 'for'.\n",
      "Start generate\n",
      "Generate text by bigrams model:\n",
      "for a man that i haue done to the time\n",
      "add_text\n",
      " father\n",
      "add_text\n",
      " son\n",
      "add_text\n",
      " nay\n",
      "add_text\n",
      " how\n",
      "add_text\n",
      " will\n",
      "add_text\n",
      " you\n",
      "add_text\n",
      " do\n",
      "add_text\n",
      " not\n",
      "Generate text by trigrams model:\n",
      "for a father son nay how will you do not\n"
     ]
    }
   ],
   "source": [
    "Generate_text_with_ngrams_model(\"for\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#main \n",
    "pathname = 'Test Data/short test data'\n",
    "corp = oneCorpusFromFiles(pathname)\n",
    "printoutcorpus(corp,pathname)\n",
    "tokenedcorp = tokenize_words(corp)\n",
    "print(tokenedcorp[:20])\n",
    "onegram = ngrams(tokenedcorp, 1)\n",
    "print(grams)\n",
    "for k, v in onegram.most_common(3):\n",
    "   print (k, v)\n",
    "tot = 0\n",
    "#onegrammodel = ngramModel(corp, 1)\n",
    "#print(onegrammodel[:20])\n",
    "#twogrammodel = ngramModel(corp, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}