{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "#@author: Kelle Clark, Andrew Florian, Xinyu Xiong\n",
    "#Created on Tue Feb  4 10:05:49 2020\n",
    "#CSCI 6040 Project 1 Text Generation\n",
    "#PHASE 1: READING IN THE CORPUS\n",
    "\n",
    "#Various folders of .txt files were created in the CSCI6040 Team Project 1 folder\n",
    "#to be used for testing our application during develpment\n",
    "#/Short Test Data\n",
    "# has 3 .txt files each about 4KB\n",
    "#/Med test Data \n",
    "# has 2 .txt files one of 119KB (Tragedy of Macbeth) and 6.5MB (big)\n",
    "#/Grande test Data (the 18-document-gutenburg-copus but with 19? files cleaned using the \n",
    "#boilerplate.ipynb -author Andrew Florian and resulting files \n",
    "#shared on Canvas in Project 1 discussion forum)\n",
    "# has 19 .txt files with a total of 11.8MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we needed the help of a few packages...import all those at once\n",
    "import langid\n",
    "import itertools \n",
    "import nltk\n",
    "import numpy\n",
    "import os\n",
    "import pandas\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "import sys\n",
    "from collections import Counter\n",
    "from math import log10\n",
    "from matplotlib.pyplot import yscale, xscale, title, plot\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#from keras.models import Sequential\n",
    "#from keras.layers import Dense, Dropout, LSTM\n",
    "#from keras.utils import np_utils\n",
    "#from keras.callbacks import ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this method will read in only the .txt files in the folderpath provided as input\n",
    "#an error will be returned if unable to read a file\n",
    "#an optional extension of the code is to determine the language of the text so that\n",
    "#we could generte text by language choice.\n",
    "\n",
    "def textByFile(folderpath):\n",
    "    textfiles = [f for f in os.listdir(folderpath) if '.txt' in f]\n",
    "    filedictionary ={}\n",
    "    \n",
    "    for f in textfiles:\n",
    "        try:\n",
    "            substring = ' '\n",
    "            with open(folderpath + '/'+ f) as filetext:\n",
    "                #print(f\"the language of file \"+f+\" is {nltk.language(filetext)}\")\n",
    "                substring = substring.join(line.strip() for line in filetext)\n",
    "                filetext.close()\n",
    "                filedictionary[f] = substring\n",
    "        except:\n",
    "            print(\"Error reading file:\" + f)\n",
    "            \n",
    "    return filedictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "#method to print out the read in file based on length of file...for files with more than 500 words\n",
    "#the method only prints out the first 50 and last 50 words\n",
    "#an execption is printed if the corpus does not exist\n",
    "\n",
    "def printoutfiles(indictionary,folderpath):\n",
    "    tempcounter = 0\n",
    "    for fileidentifier in indictionary:\n",
    "        instring = indictionary[fileidentifier]\n",
    "        tempcounter = tempcounter + len(instring)\n",
    "        try:\n",
    "            print(f\"The {fileidentifier} file in {pathname} has {len(instring)} tokens. \\n\")\n",
    "            if len(instring) > 500: \n",
    "                 print(f\"The first & last 50 tokens of this file are:\\n\" + instring[:50] + \" . . . \"+ instring[-50:]+\"\\n\")\n",
    "            else:\n",
    "                 print(f\"The tokens in file {fileidentifier} are: \\n {instring} \\n\")\n",
    "        except:\n",
    "            print(\"The string from file {f} does not exist.\\n\")\n",
    "    print(f\"The collection of all .txt files in {pathname} has {tempcounter} tokens. \\n\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the pathname for the folder containing the files to be read in is currently hardcoded\n",
    "#***if you want to change the pathname, uncomment the 2nd line and put in your pathname of choice\n",
    "#***if you want to use the test data, leave the hardcoded pathname (do not uncommnet 2nd line) \n",
    "#***there are several test data sets in our folder\n",
    "\n",
    "pathname = 'Test Data/short test data'\n",
    "#pathname = 'your choice of path here'\n",
    "filedict = textByFile(pathname)\n",
    "\n",
    "#create a fileID dictionary and use fileId for files instead of filenames\n",
    "fileIDdict = {}\n",
    "\n",
    "i = 0\n",
    "for f in filedict:\n",
    "    fileIDdict[i] = filedict[f] \n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Testset1.txt file in Test Data/short test data has 62 tokens. \n",
      "\n",
      "The tokens in file Testset1.txt are: \n",
      " The cat named Bob is DAMN good! He is the best C-A-T on Earth. \n",
      "\n",
      "The Testset2.txt file in Test Data/short test data has 296 tokens. \n",
      "\n",
      "The tokens in file Testset2.txt are: \n",
      " What do we do if there are hyphens, capitols in the middle of a word, misspellings? Do we want to keep track of words at the beginning of sentences?  Our test data need to have enough words that have high frequency.  What do we do with words like 12.exe. or http://www.weirdo.com and #9 and 1999? \n",
      "\n",
      "The Testset3.txt file in Test Data/short test data has 113 tokens. \n",
      "\n",
      "The tokens in file Testset3.txt are: \n",
      " This damn cat whose name is Bob is no good. My dog is better. Cats should not exist on Earth.  Dogs are the best. \n",
      "\n",
      "The corpus of all .txt files in Test Data/short test data has 471 tokens. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#printing out the the files from the folder either by file name or fileID\n",
    "printoutfiles(filedict,pathname)\n",
    "#printoutfiles(fileIDdict,pathname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the tokens by casefolding and removing punctuation to standardize it\n",
    "# this code only keeps regexpressions [A-Za-z]\n",
    "\n",
    "def tokenize_wordListByFile(indict):\n",
    "    words = []\n",
    "    sumofwords = 0\n",
    "    #creating a regular expression for matching\n",
    "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    fileid = 0\n",
    "    for i in range(len(indict)):\n",
    "        tokens = word_tokenize(indict[i])\n",
    "        tokens = [w.lower() for w in tokens]\n",
    "        \n",
    "        #remove punctuation from words, only include words that are A-Za-z\n",
    "        punct_free_words = [re_punc.sub('', w) for w in tokens if w.isalpha()]\n",
    "        tempstring = \" \"\n",
    "        for word in punct_free_words:          \n",
    "            tempstring = tempstring + word + \" \"\n",
    "        words.append(tempstring)\n",
    "        sumofwords = sumofwords + len(words[i])\n",
    "        \n",
    "    print(f\"The word_tokenize from NLTK identified tokenized {sumofwords} words using A-Za-z.\")\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word_tokenize from NLTK identified tokenized 423 words using A-Za-z.\n",
      "[' the cat named bob is damn good he is the best on earth ', ' what do we do if there are hyphens capitols in the middle of a word misspellings do we want to keep track of words at the beginning of sentences our test data need to have enough words that have high frequency what do we do with words like or http and and ', ' this damn cat whose name is bob is no good my dog is better cats should not exist on earth dogs are the best ']\n"
     ]
    }
   ],
   "source": [
    "#clean up corpus by file and split string between expressions involving A-Za-z\n",
    "#send in the dictionary using the fileIDs\n",
    "tokenfilelist = tokenize_wordListByFile(fileIDdict)\n",
    "print(tokenfilelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tokens in the corpus are: \n",
      "   the cat named bob is damn good he is the best on earth   what do we do if there are hyphens capitols in the middle of a word misspellings do we want to keep track of words at the beginning of sentences our test data need to have enough words that have high frequency what do we do with words like or http and and   this damn cat whose name is bob is no good my dog is better cats should not exist on earth dogs are the best   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#we have the different files tokenized, in the variable tokenfilelist\n",
    "#method below creates one corpus from the string of tokens in each file \n",
    "\n",
    "def createOneCorpus(inlist):\n",
    "    temp = \" \"\n",
    "    for i in range(len(inlist)):\n",
    "        temp = temp + inlist[i] + \" \"\n",
    "    return temp\n",
    "\n",
    "def printcorpus(instring):\n",
    "    if len(instring) > 500: \n",
    "        print(f\"The first & last 50 tokens of this corpus are:\\n  {instring[:50]} \\t ... {instring[-50:]}\\n\")\n",
    "    else:\n",
    "        print(f\"The tokens in the corpus are: \\n {instring} \\n\")\n",
    "\n",
    "tokencorpus = createOneCorpus(tokenfilelist)\n",
    "printcorpus(tokencorpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
