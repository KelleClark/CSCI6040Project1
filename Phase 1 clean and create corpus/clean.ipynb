{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Reading from: 'austen-emma.txt' . . .\n",
      "     Reading from: 'big.txt' . . .\n",
      "     Reading from: 'tragedy-of-macbeth.txt' . . .\n",
      "\n",
      "   First & last 50 characters of Corpus:\n",
      "'[Emma by Jane Austen 1816]  VOLUME I  CHAPTER I   ' . . . '. Exeunt Omnes.   FINIS. THE TRAGEDIE OF MACBETH. '\n",
      "\n",
      "   NLTK identified 66729 sentences:\n",
      "      1st sentence: [Emma by Jane Austen 1816]  VOLUME I  CHAPTER I   Emma Woodhouse, handsome, clever, and rich, with a comfortable home and happy disposition, seemed to unite some of the best blessings of existence; and had lived nearly twenty-one years in the world with very little to distress or vex her.\n",
      "\n",
      "      Last sentence: ['THE TRAGEDIE OF MACBETH.']\n",
      "\n",
      "   NLTK identified 1281513 words. Here are the first 10 words: ['emma', 'by', 'jane', 'austen', 'volume', 'i', 'chapter', 'i', 'emma', 'woodhouse']\n",
      "\n",
      "   The most common unigrams are: [('the', 85990), ('of', 44737), ('and', 43762), ('to', 34332), ('a', 24513), ('in', 24361), ('that', 14584), ('he', 14324), ('was', 13880), ('it', 13395)]\n",
      "\n",
      "   The most common bigrams are: [('of the', 13178), ('in the', 7031), ('to the', 4750), ('and the', 3474), ('on the', 2685), ('to be', 2246), ('at the', 2231), ('by the', 2066), ('it is', 2000), ('it was', 1993)]\n",
      "\n",
      "   The most common trigrams are: [('the united states', 463), ('one of the', 408), ('out of the', 271), ('he did not', 270), ('of the united', 243), ('i do nt', 223), ('that he was', 215), ('as soon as', 213), ('that it was', 212), ('i do not', 210)]\n",
      "\n",
      "   The most common quadgrams are: [('of the united states', 240), ('at the same time', 137), ('as a result of', 104), ('the commander in chief', 101), ('for a long time', 91), ('for the first time', 79), ('in the case of', 79), ('the end of the', 78), ('met with in the', 78), ('in the course of', 72)]\n"
     ]
    }
   ],
   "source": [
    "import os, re, string, nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "textFiles = [f for f in os.listdir('.') if '.txt' in f]    # get list of every file in current directory that ends with .txt \n",
    "rawCorpus=[]\n",
    "for f in textFiles:\n",
    "    try:\n",
    "        file = open(f,'rt')                                    # open each text file for reading\n",
    "        print (f\"     Reading from: '{f}' . . .\")\n",
    "        rawCorpus.append(file.read().replace('\\n', ' '))       # replace carrage returns with spaces       \n",
    "        file.close()                                           # close each file\n",
    "        corpus = ' '.join(rawCorpus)                           # make the list into a string \n",
    "    except:\n",
    "        print(\"Error reading files.\")\n",
    "    \n",
    "try:\n",
    "    if len(corpus)>100: \n",
    "        print(f\"\\n   First & last 50 characters of Corpus:\\n'{corpus[:50]}' . . . '{corpus[-50:]}'\")\n",
    "    else:\n",
    "        print(f\"\\n   Corpus is under 100 characters:\\n'{corpus}'\")\n",
    "except:\n",
    "    print(\"The corpus does not exist.\")\n",
    "\n",
    "try:    \n",
    "    sentences = sent_tokenize(corpus)\n",
    "    print(f\"\\n   NLTK identified {len(sentences)} sentences:\")\n",
    "    print(f\"      1st sentence: {sentences[0]}\")\n",
    "    print(f\"\\n      Last sentence: {sentences[-1:]}\")\n",
    "\n",
    "    tokens = word_tokenize(corpus)\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    punct_free_words = [re_punc.sub('', w) for w in tokens]\n",
    "    words = [word for word in punct_free_words if word.isalpha()]\n",
    "    print(f\"\\n   NLTK identified {len(words)} words. Here are the first 10 words: {words[:10]}\")\n",
    "\n",
    "except:\n",
    "    print(\"There is nothing to tokenize.\")\n",
    "    \n",
    "def ngrams(input, n):\n",
    "    output = {}\n",
    "    for i in range(len(input) - n + 1):\n",
    "        g = ' '.join(input[i:i+n])\n",
    "        output.setdefault(g, 0)\n",
    "        output[g] += 1\n",
    "    return output\n",
    "\n",
    "from collections import Counter\n",
    "try:\n",
    "    COUNTS = Counter(ngrams(words,1))\n",
    "    print(f\"\\n   The most common unigrams are: {(COUNTS.most_common(10))}\")\n",
    "    \n",
    "    COUNTS = Counter(ngrams(words,2))\n",
    "    print(f\"\\n   The most common bigrams are: {(COUNTS.most_common(10))}\")\n",
    "    \n",
    "    COUNTS = Counter(ngrams(words,3))\n",
    "    print(f\"\\n   The most common trigrams are: {(COUNTS.most_common(10))}\")\n",
    "    \n",
    "    COUNTS = Counter(ngrams(words,4))\n",
    "    print(f\"\\n   The most common quadgrams are: {(COUNTS.most_common(10))}\")\n",
    "except:\n",
    "    print(\"There is an error with ngrams.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
