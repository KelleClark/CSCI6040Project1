{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "#@author: Kelle Clark, Andrew Florian, Xinyu Xiong\n",
    "#Created on Tue Feb  4 10:05:49 2020\n",
    "#CSCI 6040 Project 1 Text Generation\n",
    "#PHASE 5: Smoothing the Language Models for the Corpus\n",
    "\n",
    "#Various folders of .txt files were created in the CSCI6040 Team Project 1 folder\n",
    "#to be used for testing our application during develpment\n",
    "#/Short Test Data\n",
    "# has 3 .txt files each about 4KB\n",
    "#/Med test Data \n",
    "# has 2 .txt files one of 119KB (Tragedy of Macbeth) and 6.5MB (big)\n",
    "#/Grande test Data (the 18-document-gutenburg-copus but with 19? files cleaned using the \n",
    "#boilerplate.ipynb -author Andrew Florian and resulting files \n",
    "#shared on Canvas in Project 1 discussion forum)\n",
    "# has 19 .txt files with a total of 11.8MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we needed the help of a few packages...import all those at once\n",
    "import langid\n",
    "import itertools \n",
    "import math\n",
    "import mmap\n",
    "import nltk\n",
    "import numpy\n",
    "import os\n",
    "import pandas\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "import sys\n",
    "from collections import Counter\n",
    "from math import log10\n",
    "from matplotlib.pyplot import yscale, xscale, title, plot\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#from keras.models import Sequential\n",
    "#from keras.layers import Dense, Dropout, LSTM\n",
    "#from keras.utils import np_utils\n",
    "#from keras.callbacks import ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#**** from phase 1 reading in the tokenized corpus\n",
    "\n",
    "def tokensByFiles(folderpath):\n",
    "    textfiles = [f for f in os.listdir(folderpath) if '.txt' in f]\n",
    "    tokenfilelist =[]\n",
    "    \n",
    "    for f in textfiles:\n",
    "            rawcorpus = []\n",
    "            substring = ''\n",
    "            file = open(folderpath+\"/\"+f,'rt', encoding='utf-8', errors='replace')                                   \n",
    "            print (f\"     Reading from: '{f}' . . .\")\n",
    "            rawcorpus.append(file.read()\n",
    "                     .replace('. . .','.')\n",
    "                     .replace('!',' .')  # substitue space period for ! mark to have a simple token to end a sentence \n",
    "                     .replace('\"',' ')\n",
    "                     .replace('#',' ')  \n",
    "                     .replace('$',' ')\n",
    "                     .replace('%',' ')\n",
    "                     .replace('&',' ')\n",
    "                     .replace('\\\\',' ') \n",
    "                     .replace('\\' ',' ')  # only remove ' if it has a space before or after meaning it is used as a quote\n",
    "                     .replace(' \\'',' ')  # but leave it in if it is inside a word as a contraction\n",
    "                     .replace('\\- ',' ')  # only remove - if it has a space before or after meaning it is to be left in the \n",
    "                     .replace(' \\-',' ')  # word e.g. C-A-T\n",
    "                     .replace('(',' ')\n",
    "                     .replace('\\n', ' ')        \n",
    "                     .replace(')',' ')\n",
    "                     .replace('*',' ')\n",
    "                     .replace('+',' ')\n",
    "                     .replace(',',' ')\n",
    "                     .replace('. ',' ')  \n",
    "                     .replace('/',' ')                 \n",
    "                     .replace(':',' ')\n",
    "                     .replace(';',' ')\n",
    "                     .replace('<',' ')\n",
    "                     .replace('=',' ')\n",
    "                     .replace('>',' ')\n",
    "                     .replace('?',' .')  # substitue space period for ? mark to have a simple token to end a sentence\n",
    "                     .replace('@',' ')\n",
    "                     .replace('[',' ')\n",
    "                     .replace('\\\\',' ')\n",
    "                     .replace(']',' ')\n",
    "                     .replace('^',' ')\n",
    "                     .replace('_',' ')   #  remove all unwanted punctuation\n",
    "                     .replace('`',' ')\n",
    "                     .replace('{',' ')\n",
    "                     .replace('|',' ')\n",
    "                     .replace('}',' ')\n",
    "                     .replace('~',' ')\n",
    "                     .replace('0',' ')   #  remove all digits\n",
    "                     .replace('1',' ')\n",
    "                     .replace('2',' ')\n",
    "                     .replace('3',' ')\n",
    "                     .replace('4',' ')\n",
    "                     .replace('5',' ')                \n",
    "                     .replace('6',' ')\n",
    "                     .replace('7',' ')\n",
    "                     .replace('8',' ')\n",
    "                     .replace('9',' '))            \n",
    "            file.close()\n",
    "            \n",
    "            substring = substring + rawcorpus[0]\n",
    "            #print(f\"the language of file \"+f+\" is {nltk.language(substring)}\")\n",
    "            print(f\"the estimated language of the file {f} is {langid.classify(substring)}\")\n",
    "            \n",
    "            #tokens=substring.split()\n",
    "            tokens = word_tokenize(substring)\n",
    "            tokens = [w.lower() for w in tokens]\n",
    "            tokenfilelist.append(tokens)\n",
    "             \n",
    "    return tokenfilelist\n",
    "\n",
    "\n",
    "#we have the different files tokenized, in the variable tokenfilelist\n",
    "#method below creates one corpus from the string of tokens in each file \n",
    "def createOneCorpus(inlist):\n",
    "    temp = \" \"\n",
    "    for i in range(len(inlist)):\n",
    "        for w in inlist[i]:\n",
    "            temp = temp + w + \" \"\n",
    "    return temp\n",
    "\n",
    "def printcorpus(instring):\n",
    "    if len(instring) > 500: \n",
    "        print(f\"The first & last 50 tokens of this corpus are:\\n  {instring[:50]} \\t ... {instring[-50:]}\\n\")\n",
    "    else:\n",
    "        print(f\"The tokens in the corpus are: \\n {instring} \\n\")\n",
    "\n",
    "#ngrams returns a dictionary\n",
    "# enumerate ngrams code copied from Eisentein and CSCI6040 ipynb\n",
    "# returns the ngram from instring and n\n",
    "def ngrams(instring, n):\n",
    "    outset = {}\n",
    "    for i in range(len(instring) - n + 1):\n",
    "        g = ' '.join(instring[i:i+n])\n",
    "        outset.setdefault(g, 0)\n",
    "        outset[g] += 1\n",
    "    return outset        \n",
    "\n",
    "#### modified from phase 2, The team kept both ngrams method and newngram method for computing the \n",
    "###unigrams models....\n",
    "###output of this newngram is a dictionary obj ...\n",
    "\n",
    "#!!!newngram now returns a dictionary object \n",
    "def newngram(toks, n):\n",
    "    output = {}   \n",
    "    for i in range(len(toks) - n + 1):\n",
    "        g = ' '.join(toks[i:i+n])\n",
    "        output.setdefault(g, 0)\n",
    "        output[g] += 1\n",
    "    \n",
    "   \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Reading from: 'Testset1.txt' . . .\n",
      "the estimated language of the file Testset1.txt is ('en', -112.47618627548218)\n",
      "     Reading from: 'Testset2.txt' . . .\n",
      "the estimated language of the file Testset2.txt is ('en', -757.8414204120636)\n",
      "     Reading from: 'Testset3.txt' . . .\n",
      "the estimated language of the file Testset3.txt is ('en', -295.17291164398193)\n",
      "[['the', 'cat', 'named', 'bob', 'is', 'damn', 'good', 'he', 'is', 'the', 'best', 'c-a-t', 'on', 'earth'], ['what', 'do', 'we', 'do', 'if', 'there', 'are', 'hyphens', 'capitols', 'in', 'the', 'middle', 'of', 'a', 'word', 'misspellings', '.', 'do', 'we', 'want', 'to', 'keep', 'track', 'of', 'words', 'at', 'the', 'beginning', 'of', 'sentences', '.', 'our', 'test', 'data', 'need', 'to', 'have', 'enough', 'words', 'that', 'have', 'high', 'frequency', 'what', 'do', 'we', 'do', 'with', 'words', 'like', '.exe', 'or', 'http', 'www.weirdo.com', 'and', 'and', '.'], ['this', 'damn', 'cat', 'whose', 'name', 'is', 'bob', 'is', 'no', 'good', 'my', 'dog', 'is', 'better', 'cats', 'should', 'not', 'exist', 'on', 'earth', 'dogs', 'are', 'the', 'best']]\n",
      "The tokens in the corpus are: \n",
      "  the cat named bob is damn good he is the best c-a-t on earth what do we do if there are hyphens capitols in the middle of a word misspellings . do we want to keep track of words at the beginning of sentences . our test data need to have enough words that have high frequency what do we do with words like .exe or http www.weirdo.com and and . this damn cat whose name is bob is no good my dog is better cats should not exist on earth dogs are the best  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#**** from phase 1 reading in the .txt files and creating the tokenized corpus\n",
    "pathname = 'Test Data/short test data'\n",
    "##pathname = 'your choice of path here'\n",
    "\n",
    "#read in the corups file by file\n",
    "tokenfilelist = tokensByFiles(pathname)\n",
    "print(tokenfilelist)\n",
    "\n",
    "tokencorpus = createOneCorpus(tokenfilelist)\n",
    "printcorpus(tokencorpus)\n",
    "tokens = tokencorpus.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sum of all the probabiities of unigrams needs to be 1 and it is 1.0000000000000009\n",
      "\n",
      "Number of rows in Unigram Prob. Model :  63\n",
      "           prob.\n",
      "the     0.052632\n",
      "cat     0.021053\n",
      "named   0.010526\n",
      "bob     0.021053\n",
      "is      0.052632\n",
      "...          ...\n",
      "cats    0.010526\n",
      "should  0.010526\n",
      "not     0.010526\n",
      "exist   0.010526\n",
      "dogs    0.010526\n",
      "\n",
      "[63 rows x 1 columns]\n",
      "the unigram of greatest freq is: do \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1a202ee890>]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#**** from phase 2 creating the unigram language models using ngrams:\n",
    "#unigram prob. model using prob(x) = (frequency of x in corpus)/(total in corpus)\n",
    "\n",
    "def createUnigramModel(instring):\n",
    "    n = 1\n",
    "    outset = word_tokenize(instring)\n",
    "    totalpossible = len(outset)\n",
    "    \n",
    "    anoutcome = newngram(outset,n)\n",
    "    probmodel = anoutcome\n",
    "    sumofprob = 0\n",
    "    \n",
    "    for keyword in anoutcome.keys(): \n",
    "        probmodel[keyword] = (probmodel[keyword]) / totalpossible\n",
    "        sumofprob = sumofprob + probmodel[keyword]\n",
    "        \n",
    "    print(f\"The sum of all the probabiities of unigrams needs to be 1 and it is {sumofprob}\\n\")\n",
    "    return probmodel\n",
    " \n",
    "#create the unigram model \n",
    "unigrammodel = createUnigramModel(tokencorpus)\n",
    "\n",
    "\n",
    "pandas.set_option(\"display.max_rows\", 10)\n",
    "unidataframe = pandas.DataFrame.from_dict(unigrammodel, orient = 'index', columns = ['prob.'])\n",
    "print('Number of rows in Unigram Prob. Model : ', len(unidataframe.index))\n",
    "print(unidataframe)\n",
    "\n",
    "#Attempt to try and plot the unigram language model using first a Counter object\n",
    "COUNT = Counter(unigrammodel)\n",
    "greatestprob = 0\n",
    "bigword = ''\n",
    "for w in COUNT.keys():\n",
    "    if COUNT[w] >= greatestprob:\n",
    "        bigword = w\n",
    "        greatestprob = COUNT[w]\n",
    "        \n",
    "print(f\"the unigram of greatest freq is: {bigword} \\n\")\n",
    "probbigword = COUNT[bigword]\n",
    "yscale('log'); xscale('log'); title('Frequency of n-th most frequent word and 1/n line.')\n",
    "##RAN INTO SOME ISSUES GETTING THE GRAPH TO PRINT THE RANK ORDER OF THE WORDS...\n",
    "##BUT WHAT I THINK THIS IS SHOWING IS THAT IF WE WANT TO SMOOTH THE PROB. MODEL FOR\n",
    "##UNIGRAMS, WE COULD USE PROB. M/i for the ith rankend term and M is the frequency of the\n",
    "##MOST COMMON UNIGRAM\n",
    "plot([c for (w,c) in COUNT.most_common()])\n",
    "plot([probbigword/i for i in range(1, len(COUNT)+1)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Reading from: 'Testset4.txt' . . .\n",
      "the estimated language of the file Testset4.txt is ('en', -589.058343410492)\n",
      "[['what', 'do', 'we', 'do', 'if', 'there', 'are', 'not', 'enough', 'cats', 'in', 'the', 'world', '.', 'i', 'am', 'hungry', 'and', 'so', 'is', 'my', 'cat', 'the', 'sun', 'is', 'shining', 'and', 'the', 'sky', 'is', 'blue', 'today', 'is', 'a', 'shadow', 'of', 'yesterday', '.']]\n",
      "Number of rows in Unigram Prob. Model w/o test data:  63\n",
      "           prob.\n",
      "the     0.052632\n",
      "cat     0.021053\n",
      "named   0.010526\n",
      "bob     0.021053\n",
      "is      0.052632\n",
      "...          ...\n",
      "cats    0.010526\n",
      "should  0.010526\n",
      "not     0.010526\n",
      "exist   0.010526\n",
      "dogs    0.010526\n",
      "\n",
      "[63 rows x 1 columns]\n",
      "Number of rows in Unigram Prob. Model w/ test data :  75\n",
      "              prob.\n",
      "the        0.052632\n",
      "cat        0.021053\n",
      "named      0.010526\n",
      "bob        0.021053\n",
      "is         0.052632\n",
      "...             ...\n",
      "sky        0.007692\n",
      "blue       0.007692\n",
      "today      0.007692\n",
      "shadow     0.007692\n",
      "yesterday  0.007692\n",
      "\n",
      "[75 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "####****** Taking one file in from the gutenburg corpus outside our training set of the 18-document-gutenberg-corpus\n",
    "####****** we tokenize the text and create a set of the unigrams and quadgrams in the test set.\n",
    "#**** from phase 1 reading in the .txt files and creating the tokenized corpus\n",
    "pathname = 'Test Data/short test2 data'\n",
    "#pathname = 'your choice of path here'\n",
    "\n",
    "#read in the corups file by file\n",
    "testtokenfilelist = tokensByFiles(pathname)\n",
    "print(testtokenfilelist)\n",
    "testtokencorpus = createOneCorpus(testtokenfilelist)\n",
    "testtokens = testtokencorpus.split()\n",
    "\n",
    "\n",
    "#####*******update the model so that tokens that are in the test text not in the training\n",
    "#####****** set have nonzero value...but something really small...use M = 1/(tot # in test + tot in training)\n",
    "traintestunimodel = unigrammodel\n",
    "M = 0\n",
    "\n",
    "for w in testtokens:\n",
    "    M = M + 1\n",
    "    if w not in unigrammodel.keys():\n",
    "        traintestunimodel.update({w: 0})       \n",
    "\n",
    "for w in unigrammodel.keys():\n",
    "    M = M + math.ceil(unigrammodel[w]*len(unigrammodel))\n",
    "\n",
    "for w in traintestunimodel.keys():\n",
    "    if (traintestunimodel[w] == 0):\n",
    "        traintestunimodel[w] = 1/M \n",
    "        \n",
    "print('Number of rows in Unigram Prob. Model w/o test data: ', len(unidataframe.index))        \n",
    "print(unidataframe) \n",
    "\n",
    "traintestunidataframe = pandas.DataFrame.from_dict(traintestunimodel, orient = 'index', columns = ['prob.'])\n",
    "print('Number of rows in Unigram Prob. Model w/ test data : ', len(traintestunidataframe.index))\n",
    "print(traintestunidataframe)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For our unigrammodel of the language:\n",
      "\n",
      "The number of bits we would need to find the correct last word of given phrase is our entropgy H = 6.398616581666743\n",
      "The perplexity for the unigrammodel is 2^H = 84.3675663730743\n"
     ]
    }
   ],
   "source": [
    "#####***** entropy calculation then perplexity for our basic unigram model of the test data:\n",
    "####***** we will define our vocabulary to be all words in training + test set\n",
    "####***** Then the idea is that entropy for the model, or likelyhood that the\n",
    "####***** model would correctly predict the last word of a string of text in the form of test data\n",
    "####***** is related to -(the sum over all unigrams in vocubulary x_i of prob(x_i)*log(prob(x_i))\n",
    "TRAINTESTCOUNT1 = Counter(traintestunimodel)\n",
    "traintestvocprob = [TRAINTESTCOUNT1[w] for w in TRAINTESTCOUNT1.keys()]\n",
    "\n",
    "traintestentropysum = 0\n",
    "for prob in traintestvocprob:\n",
    "    traintestentropysum = traintestentropysum - (prob * math.log(prob, 2))\n",
    "print(\"For our unigrammodel of the language:\\n\")\n",
    "print(f\"The number of bits we would need to find the correct last word of given phrase is our entropgy H = {traintestentropysum}\")\n",
    "traintestperplexity = (2**traintestentropysum)\n",
    "print(f\"The perplexity for the unigrammodel is 2^H = {traintestperplexity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lamd:  [1]\n",
      "the sum of the linear smoothed unigram model is 1.0000000000000009\n",
      "Number of rows in Linear Smoothed Unigram Prob. Model :  63\n",
      "           prob.\n",
      "the     0.052632\n",
      "cat     0.021053\n",
      "named   0.010526\n",
      "bob     0.021053\n",
      "is      0.052632\n",
      "...          ...\n",
      "cats    0.010526\n",
      "should  0.010526\n",
      "not     0.010526\n",
      "exist   0.010526\n",
      "dogs    0.010526\n",
      "\n",
      "[63 rows x 1 columns]\n",
      "the unigram of greatest freq in the smoothed unigram model is: do \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAf0AAAEMCAYAAADOAm80AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3dd5gUVdrG4d87MyQJAwooDCAqiCIgyogYwQCSQVFRMYAKImBaXT9X3V3X1XXXsCaQKCKYRUUxR0SC6CAGEANiABQEBJQk6Xx/nGK3mZ3M9Jzu6ee+rr5guqurnq709qk+VWXOOURERKT8SwsdQERERMqGir6IiEiKUNEXERFJESr6IiIiKUJFX0REJEWo6IuIiKQIFf1SYmZ7m9l0M/vNzO6K0zS+M7OT4zHueDKzW8xslZktD50lNDNbYGYdQufYHWZ2nJl9WcL39jezGTF/rzez/UsvXflSmtu8mU0ws1tKY1zFnG5jM3NmllGEYXdZP+Kca5qZXVwW08o13aDbQKFFP1rpNkXBdj7ql0W4JDMIWAXUcM5dvbsjC7WBFle0MTcp4PWGwNVAc+fcPmWXbPeZ2U1m9kghwwwzsxwz+93MJhQ2TufcIc65aUUdfwiFFRrn3HvOuWalMS3nXDXn3OLSGFeyS5ZtPhWYWQszey1qrOR5MRszq29mS3d3WmW9DRS1pd8jCrbz8WPuAYryLa6c2xf43OlqR7ntC6x2zv2c14vlYL35EbgFGB86iBRNOVjnJP62Ak8BFxUwTFfg1bKJU4qccwU+gO+Ak/N4vjHg8DPlB2B69Hw7YBawFvgE6BDznv2Ad4HfgDeA4cAj0WsdgKX5TRv/BeU64BtgNX6B7JkrywVRllXADTHjSQeuj977GzAXaAiMAO7KNc2pwJX5zIujgQ+BddG/R0fPT8CvJFuA9fnMrwnR9F6KMswBDshnOoNyjW9qzPy4Bvg0yvAkUDmfcfQHZgJ3R8ticZS/P7AE+Bm4IGb4TGAisBL4HrgRSIteaxItt3XRvH0yen56NN83RDn75spwMrAJ2BG9PiFZ1hugczT/t0bZPylkO7kFmFDU7Sm/8UfL4UHgJ2BZNN70Ei7TrsDn0XxbBlwT81p34ONoPLOAVtHzk6LltSnKdW0en2GXeU7x18sZMX87oElRthHgoGj5/wJ8CZwZ81o3YB7wazQvbipsX5UrV23gxWh+/AK8x3/X/++AP0afb0O0fPYGXolyvgnUihlXT2BBNK5pwMExrx0cPbc2Gqbn7mzz+S3H6LXDgI+ijE8CTwC3lMH+Ih24E789LQaGRvM/o4jr+Iy8MkavPw0sj+bFdOCQou5jgY7AF9F7h+P3KRcXsr02AVw+rz0LnJYo20BRH6VR9CcCVYEqQBZ+x9oVv7PtGP1dJ3rPbODfQCXg+OhDFXXnfSXwPtAgev9o4PFcWcZGOQ4Ffifa2PAb7GdAM8Ci1/cC2uJbajtX1trARmDvPD7vnsAa4DwgAzg7+nuvmIWV5wYV8/ov0TQzgEeBJwoZ/pY85scHQP0oz0JgcAEr1jZgAH4jvAW/wxsRzb9O0fyvFg0/EXgeqB7Nz6+Ai6LXHgduiJZpZeDYvFbYfHLsslyTbL25aed0irCdFKvo5zd+YEqUsSpQN1rel5Rwmf4EHBf9vxZwePT/w/E78SOj8VwQ5apU0DZfwDL9juKtlwXt8PLcRqL5sST67BnRZ1hFtNOPMrWM1p9WwAqgd37rXB65bgNGARWix3GAxXy+9/GFPiuadx/hi2ol4G3gr9GwB+K/GHSMxnMtsAioGP29CN8AqQicGC2vZiXZ5gtajtH4vweuiqZ7Ov5LRUFFv7T2F4PxxbVhlPkddi36ha3jBRX9C6NpVgLuAT4uyj4Wv2//NZoPFaL5so0SFv1oHKuA6omyDRT1UdSivx7/7W8tMCXXhrR/zLD/B0zK9f7XopWxUTSTq8a89hhF33kvBE6Kea0efiXOiMnSIOb1D4Czov9/CfTK5/MtBDpG/x8GvJzPcOcBH+R6bjbQP78NNtewE4BxMX93Bb4oZPi8dgDnxvx9OzCqgBXr65i/W0bzaO+Y51YDrfEb+e/43913vnYJMC1mAx8TO3/zWmHzybHLck2y9eYmyrDo44vK78QUJfyXy3eKu0yj//8QLccauTKMBP6e67kvgfa5MxZxmRZ3vSxoh5fnNgL0Bd7LNa7RRMU2j+ncA9yd3zqXx/A344vY/6zL0efrF/P3M8DImL8v47/7xT8DT8W8loZvzXbAf5FYTtTIiF5/nOioBMXc5gtajvgvxz8SfXGJXpuVe/y5lktp7S/eJqbg4b8wOPw2V5R1PN+inytzzWi8mUVYf84H3o95zYCllLzonwS8lcjbQH6Pov6m39s5VzN69M712pKY/+8LnGFma3c+gGPxO9r6wBrn3IaY4b8v4vR3jvu5mPEuBLbjV6KdYnuHbwSqRf9viD+8m5eHgXOj/5+LP7yZl/p55P0e/82/qPLMZ2bXx3SSHFWSceRjRcz/NwE453I/Vw3/LXhny2Cn2M92LX4j+SDqfX5hIRmLIhnWm91iZq/ELNd+RcxaAfgpJu9ofGtop6IuU4A++J3G92b2rpkdFTOdq3PN74b4eV1SpTUP8xvPvsCRuTL3A/YBMLMjzewdM1tpZuvwrc3auca9hPzdgW+Fv25mi83sulyv557H+c3zXfYTzrkd0XSzoteWRM/tVJR9SEHzJL/lWB9Y5qLKEDOtgpTW/qI+u87r2OGKso7nyczSzeyfZvaNmf2KL7Sw63LOb17tkimaLwWtD4XpCryc67mg20BRlUaHltiVagm+xTYw90Bmti9Qy8yqxuzAG8W8fwOwR8zw6UCdXOO+0Dk3M49xNy4k4xLgAGB+Hq89Asw3s0Pxv7dNyWccP+JneqxGlEJHDufcP4B/5H56d8dbDKvwrd998b8Bg/9sywCcc8uBgQBmdizwpplNd84t2o1pJsN6s1vLwDnXpZjjX4JvBdV2zm3bnWlH0/8Q6GVmFfBHsZ7CF4UlwK3OuVuLmCsRLAHedc51zOf1x/C/03Zxzm02s3v436Kf7+dyzv2GP8vkajM7BHjHzD50zr1VzJw/4lvJAJiZ4ef5MvyXzYZmlhZT+BvhD40XmC8f+S5HM2sPZJmZxRT+RuTf+CmOAvcX+J+VGsYM3yhX5pKu4+cAvfB9Yr7D9w1Yg2+QFGaXTDHLpaS6AqfuxvtLorBtoEhK+zz9R4AeZnZK9K2sspl1MLMGzrnvgRzgb2ZWMSoePWLe+xVQ2cy6RTupG/G/2+w0Crg1KgKYWR0z61XEXOOAv5tZU/NamdleAM65pfhOeZOAZ5xzm/IZx8vAgWZ2jpllmFlfoDm+8088rADK5NxN59x2fEG41cyqR/P4D/jliZmdYWYNosHX4HdO20sxZ6KuNyuAxmaW73YSrQuV8Yc8d2Yv6pfpXcbvnPsJeB24y8xqmFmamR0Q7cCLJZpX/cws0zm3Ff975s5lNhYYHLWOzcyqRvOvekyuRDt3/kX89neemVWIHkeY2cHR69WBX6KC3xZfIIrMzLqbWZOoGOycV9sLeVtengK6mdlJ0fp4Nb7IzcJ3ytoAXBvl74Bfl5+I3lvc+V7QcpyN/1ns8mgdPQ3/O/FuK2x/Eb12uZk1MLNa+I60O9+7O+t4dfy8XI3/op+7oVSQl4BDzOy0aPu8nAJayNH8rIw/okG0XVeK/r8fvv/LF8WYfmkobBsoklIt+s65JfhvYtfje3UuwXei2zmdc/CdTn4B/or/rXjne9cBQ/AFehl+44g9B/Je4AX84bff8B1rjixitH/jV8TX8Rv0g/iOWzs9jP92nt+hfZxzq/E9Za/Gr3TXAt2dc6uKmKG4HgSamz+Mk9/Rh9J0GX6eLwZm4FtOO09DOwKYY2br8cvgCufct9FrNwEPRznPLMmEE3i9eTr6d7WZfZTPMDfiD3teh/95aFP0XEnHfz5+R/M5/gvWZPzPHCVxHvCd+UOhg6N8OOdy8EduhkfTWIT/nXGn24Abo2V6TQmnXaqilngn4Cx8a3o58C/++wVvCHBztIz/gt/ei6Mpvhf+enzBfMBF11MoZs4v8fP5fnyLuAf+lOctzrkt+J79XaLXHgDOjykexdrmC1qO0bROi/5eg/89+Nnifp4CFLS/GIvvk/MJvsNj7umWdB2fiP+pYFn03veLGjbaT58B/BO//26KP1shP/vit+UF0d+b8P0lwJ8pkvvQftwVtg2Y/5n4lcLGY86FO5JnZjfhOzCcW9iwcc5xPP5bauNcv7dJAkqU9UZEUo+ZvQwMd86VeeEvDSl/Gd7oENwV+B6TKvgiIlKQafjTEJNSShf96LeQtfhDS/cEjiMiIgnOOXd7AX2/El7Qw/siIiJSdlK6pS8iIpJKVPRFRERShO42FUe1a9d2jRs3Dh1DRCSpzJ07d5Vzrk7hQ0pxqejHUePGjcnJyQkdQ0QkqZhZcS61LcWgw/siIiIpQkU/Dsysh5mNWbduXegoIiIi/6GiHwfOuanOuUGZmZmho4iIiPyHir6IiEiKUNEXERFJESr6IiIiKUKn7MXRr5u38sbnK0LHCC4j3Thq/72oXCE9dBQRkZSmoh9H36/eyMCJOk8f4NAGmYw+L5t9MiuHjiIikrJ0w504OuTQw9yTL08LHSO4r3/+jRufm88elTIYfV4bDm9UK3QkEUlgZjbXOZcdOkd5pJZ+HFWpkE6LLJ221yIrk+b1Mrl44oecNfp9bj21BWdkNwwdS0Qk5agjXxzo4jz/q9k+1Xlh6LFkN67FHyd/yt+mLmDb9h2hY4mIpBQV/TjQxXnyVqtqRSZe2JYBxzTmoZnf0f+hD1m7cUvoWCIiKUNFX8pURnoaf+1xCLef3ooPvv2FXiNm8tWK30LHEhFJCSr6EsSZ2Q15fFA7Nvy+nVNHzOT1BctDRxIRKfdU9CWYNvvWYuplx3BA3WoMmjSX+9/6Gp1NIiISPyr6ElS9zCo8dclR9G5dn7ve+Iphj81j45ZtoWOJiJRLKvoSXOUK6dzdtzXXdz2IV+b/RJ+Rs1m6ZmPoWCIi5Y6KviQEM2PQ8Qcwvv8RLF2zkZ7DZzJn8erQsUREyhUVfUkoHZrVZcrQY6i5RwX6jZvDpPe/Dx1JRKTcUNGXhHNAnWpMGXoMxzWtzZ+nzOf65z5jyzZdyEdEZHep6EtCqlG5AuMuOILB7Q/gsTk/cO64Oaxa/3voWCIiSU1FPw50Gd7SkZ5mXNflIO49qzWfLF1Lr+Ezmb9M81REpKRU9ONAl+EtXb1aZzF58NHscI7TR81i6ic/ho4kIpKUVPQlKbRskMkLw46lRf1MLnt8Hne89gU7duhCPiIixaGiL0mjTvVKPDrwSM46oiEj3vmGQZNy+G3z1tCxRESShoq+JJVKGencdlpLbu51CO98uZJTH5jFt6s2hI4lIpIUVPQl6ZgZ5x/VmEkXtWX1+t/pNXwG079aGTqWiEjCU9GXpHX0AbV5Ydix1K9Zhf4PfcC49xbrhj0iIgVQ0Zek1nDPPXjm0qPp2HxvbnlpIdc8/Smbt24PHUtEJCGp6EvSq1opg5H92nDlyU155qOl9B3zPit+3Rw6lohIwlHRl3IhLc248uQDGXXu4Xy94jd63D+DeT+sCR1LRCShZIQOIFKaOreoR+PaVRk4MYfTR82masX00JFKRaUK6dzepxUnHFQ3dBQRSWKmjk/xk52d7XJyckLHSEm/bNjC+Bnfsv73baGjlIrpX61k09btvHV1e/aoqO/qUr6Z2VznXHboHOWR9h5SLu1ZtSLXnNIsdIxSk/PdL5w+ajYjp33D1Z3Kz+cSkbKl3/TjQDfckdKW3XhPereuz+jpi/lh9cbQcUQkSanox4FuuCPx8KeuB5ORZvz9pc9DRxGRJKWiL5Ik9q5RmctObMobn6/gXV2BUERKQEVfJIlceGxj9qtdlb9NXcCWbTtCxxGRJKOiL5JEKmWk85fuzVm8cgMTZn0bOo6IJBn13o+nLevh+1kle296Jah3KKRrEcmuTjioLiceVJd73/ya3q2zqFujcuhIIpIkdJ5+HGXXT3c5g6qVfARV68KhfaH1uVD3oNILJknvu1Ub6HT3dHocWp+7zjw0dByRUqXz9ONHzch42qsJnD+6ZO/duBo+ewbeHwmz7oesNtC6H7ToA1Vqlm5OSTqNa1flouP2Y+S0bzjnyEa02bdW6EgikgTU0o+jUrki3/qf4dOn4ONH4efPIaMyHNQdDusH+7WHtPJxmVkpvg2/b+PEu6ZRt3plnh96DGlpFjqSSKlQSz9+1JEv0VWrC0cPg0tnwcB34LBzYdEbMOlUuKcVvPV3WP1N6JQSQNVKGVzf9WA+W7aOp3KWhI4jIklALf04itu197duhi9f9q3/b94GtwMaHe1b/817Q6Xd6EcgScU5x5mjZ/PNyg28c00HMqtUCB1JZLeppR8/auknowqVocVpcO4zcNUCOOkvsH4FPD8U7jwQpgyB72aCvtCVe2bGTT0PYe3GLdz9xleh44hIglPRT3Y16sNxV8Nlc+HC1/yXgc+fhwld4b7D4N07YK0O/ZZnh9TP5Oy2jZj0/vd8ufy30HFEJIHp8H4cBbu17pYNsHAqzHsEvnsPMNi/vT/17+DuUKFK2WeSuFqzYQsd7pxG83o1eGzgkZipU58kLx3ejx+19MujilXh0LOg/4twxSfQ/v9g9WJ49mK4sxlMvRKW5ujwfzlSq2pFrul0ILMXr+aV+ctDxxGRBKWWfhwFa+nnZccO3+r/+FH4/AXYtglqN/Od/1qdBdX3Dp1QdtP2HY7u98/g101befMP7alSUadzSnJSSz9+VPTjKKGKfqzNv8KC5/wXgCVzwNKhaUdofQ4c2AUyKoZOKCU0Z/Fq+o55n+Oa1qbRnnuEjpNQKqSnMfD4/cmqqZ+3Ep2KfvzoinypqHINaHOBf6z62hf/T56Ar16FKntCqzP91f/qtQqdVIrpyP334pL2+/PM3KUs/OnX0HESyqr1W6hboxJDOjQJHUUkGLX04yhhW/p52bHdn/M/7xF/DYDtW2Cflr7zX8szoOpeoROK7JbWN79Ot5b1uPXUlqGjSCHU0o8ftfTFS4sO8TftCBt/gfnP+C8Ar/4fvH4jNOvirwZ4wEm6858kpayaVVi2dlPoGCJBae8dB2bWA+jRpEmSHkbcY09oO9A/ViyAeY/Cp0/Cwheg2t7Qqq//AlCnWeikIkWWVbMK367aEDqGSFA6ZS8OnHNTnXODMjMzQ0fZfXsfAp3/AX9YCH0fhaxseP8BGNEWxp4EOeNh87rQKUUKlVXLt/T1k6akMhV9KZqMiv7CPmc/5r8AdLoVtm6EF6/yl/595mL45h1/aqBIAsqqWYWNW7azduPW0FFEgtHhfSm+nXf+O2oo/DjP9/7/7Gn/yGwIh54Nrc+GPfcPnVTkPxrU8qfqLVu7iVpVdVqqpCa19KXkzCDrcOh2F1z9FZw+HmofCNPv8Nf9f6ir7w/w+/rQSUXIqumvW6DOfJLKVPSldFSoDC36wHnP/vfOf78th+eHwF3N4OVrYc13oVNKCqtfszIAy9ao6EvqUtGX0peZteud/w7u6Tv83XcYTL4Qfvw4dEJJQXtWrUjlCmlq6UtKU9GX+DGDRu3g1JFw5adw1DD4+g0Y0x4e7gmL3tRNf6TMmJk/V18tfUlhKvpSNmrUh05/h6vmQ8ebYdVX8EgfGHUcfPIkbFePaom/rFp7qKUvKU1FX8pW5Uw45gq44lPo9QDs2ArPDYJ7W8PsB9TpT+JKV+WTVKeiL2FkVPS39b10NpzzFNTaF177E9zdHN66GX5bETqhlEMNalXhlw1b2LhlW+goIkGo6EtYaWlw4Ckw4GW4+C3Yrz2892+4pwW8cLm/C6BIKdl5W90f1dqXFKWiL4mjQTb0neR7/R92rr/d7/Aj4Il+8MOc0OmkHMj6zwV6NgdOIhKGir4knr0OgO53+/P9j/8jfD8TxneCB0+BL17SpX6lxHa29NWDX1KVir4krmp14MQbfPHvcjv89iM8cY6/2c/ch2GrWmtSPHWrVyI9zVi2dmPoKCJBqOhL4qtYFY68BC6bB30ehApVYOrlcG8reO8u2LQmdEJJEhnpaexTo7Ja+pKyVPQleaRnQMvT4ZLpcP7z/ra/b90Md7eAV6+HtUtCJ5QksPMWuyKpSEVfko8Z7N8BznsOBs+AZl1hzii4rzU8OwiWzw+dUBJYA12VT1KYir4kt31aQp+xcMXH0HYQLHwRRh0Dk06Dxe/qMr/yP7JqVWH5r5vZul0dQiX1qOhL+VCzEXS+Df6wAE78Myz/DCb2hPGnwDdvq/jLf2TVrMIOB8vXqSOopB4VfSlfqtSC46+BKz+DbnfBuqUw6VQY3xm+eUfFX2LO1dchfkk9KvpSPlWoDEdcDJfPi4r/EpjUW8VfdFU+SWkq+lK+ZVT6b/Hveies/cEX/4e6wOJpKv4pqL4u0CMpTEVfUkNGJWg70Hf463onrPkeJvaCh7qqw1+KqVwhndrVKurwvqQkFX1JLTuL/+XzoMsdsOZb3+FvQjf49r3Q6aSM6Ba7kqpU9CU1VagMRw6Cyz/2l/hd/Q083B0e6gbfzQidTuIsq5bO1ZfUpKIvqa1CZX+J3ys+gc7/gtWLfKt/Qnf4bmbodBInO1v6Tj/rSIpR0RcBX/zbDfa/+Xf+F6z6CiZ09cX/+1mh00kpy6pZhd+37WDV+i2ho4iUKRV9kVgVqkTF/xPo/E9f/B/qAg/3UPEvR7Jq7QHoXH1JPRmhAyQTM+sNdAPqAiOcc68HjiTxUqEKtLsU2vSHnIdgxt2++O/XHk64ARodGTqh7Iad5+qPnb6YxrX3CJymfGpeL5NureqFjiG5pEzRN7PxQHfgZ+dci5jnOwP3AunAOOfcP/Mbh3NuCjDFzGoBdwIq+uVdhSpw1BBf/Oc+BDPugfGd4MAucNKf/Z3+JOnsV7sqWTWr8NqC5aGjlFu9D8tS0U9AliodWczseGA9MHFn0TezdOAroCOwFPgQOBv/BeC2XKO40Dn3c/S+u4BHnXMfFTTN7Oxsl5OTU6qfQwLbssHf0W/GvfD7r9DqTOjwJ9hzv9DJRMoNM5vrnMsOnaM8SpmWvnNuupk1zvV0W2CRc24xgJk9AfRyzt2GPyqwCzMz4J/AK4UVfCmnKlaF466GNgNg5r3+C8D8Z6HNBXD8H6H6PqETiojkK9U78mUBS2L+Xho9l5/LgJOB081scF4DmNkgM8sxs5yVK1eWXlJJLHvsCR3/5s/zP/w8mDsB7jsM3vwbbFobOp2ISJ5SvehbHs/l+3uHc+4+51wb59xg59yofIYZ45zLds5l16lTp9SCSoKqUQ+63w1DP4CDusGMf8O9reC9f8OWjaHTiYjsItWL/lKgYczfDYAfA2WRZLbXAdBnHAyeAQ3bwVt/8y3/D8fB9q2h04mIACr6HwJNzWw/M6sInAW8EDiTJLN9WkK/p2DAq75z30tXw/Bs+PQp2LEjdDoRSXEpU/TN7HFgNtDMzJaa2UXOuW3AMOA1YCHwlHNuQcicUk7sexQMeAXOeRoqVodnB8Lo4+DLV3VHPxEJJmVO2StLZtYD6NGkSZOBX3/9deg4EtqOHbDgWXj7Fn9Xv4btoOPNusCPSD50yl78qOjHkc7Tl11s3wrzJsG0f8L6FXBwTzjpr1C7SehkIglFRT9+Uubwvkhw6RUg+0K4fJ6/lO83b8MDR8JL18B6nd4pIvGnoi9S1ipWhfbX+uLfpj/kjIf7WsO7d/gr/omIxImKvkgo1epCt7tg6Bw44AR45xa473CY+zBs3xY6nYiUQyr6cWBmPcxszLp160JHkWRQuyn0fQQufA1qNoKpl8OoY9TTX0RKnYp+HDjnpjrnBmVmZoaOIsmkUTu46HU4c5Lv9Pd4X5jQHZbNDZ1MRMoJFX2RRGIGzXv6Q/5d74SVX8DYE+HpAfDLt6HTiUiSU9EXSUTpFaDtQLjiYzj+WvjqVRjRFl6/UTf0EZESU9EXSWSVqsOJN8Blc6HlmTBruL+m/5wxuqa/iBSbir5IMqhRH3qPgEumwz4t4JU/wgNHwZevqLOfiBSZir5IMqnXCs5/Ac5+wv/9+FkwsSf89GnYXCKSFFT040Cn7ElcmUGzLjBkNnS5A5bPh9HHw5Sh8OtPodOJSALTtffjSNfelzKxaS28dyfMGQ1pGXDMFXD0Zf7KfyJJSNfejx+19EWSXZWa0OkWGPoBNO0E026D+9vAx4/7O/yJiERU9EXKiz33gzMfhgtfh+r1YMpgGHcS/DAndDIRSRAq+iLlTaMj4eK34NTR8NtPML4TTL4Q1v4QOpmIBKaiL1IepaXBoWf58/vb/x988RIMPwLevgV+Xx86nYgEoqIvUp5VrAonXA/DcuDgHjD9juj3/sf0e79IClLRjwOdsicJp2ZD6DMOLnoDMrNgyqUw9gT44f3QyUSkDKnox4HusicJq2FbuOhNOG0srP8Zxp8Cky+CdUtDJxORMqCiL5Jq0tKg1ZlwWY6/mc8XL8L92TDtX7BlY+h0IhJHKvoiqapiVX8zn6EfwIGnwLR/+Dv5zX9W1/MXKadU9EVSXa19/fn9/V+CyjVh8gB4qCv89EnoZCJSylT0RcRrfCxc8i50vwdWfQmj28MLl8P6laGTiUgpUdEXkf9KS4fsAXDZR9BuCHz8qD/F7/2RsH1r6HQisptU9EXkf1WpCZ3/AZfOggZt4NXrYNSxsHha6GQishtU9EUkf3WawbnPwlmPwdZNMLEXPHkurPk+dDIRKQEV/TjQxXmkXDGDg7r5Xv4n3giL3vK9/N/5h07xE0ky5nRqTtxkZ2e7nJyc0DFESte6ZfDGX2D+ZKjRAE65BZr39l8OREqBmc11zmWHzlEeqaUvIsWTmQWnPwgDXoEqteDp/jCxJ/y8MHQyESmEir6IlMy+R8OgadD1TvjpUxh5DLz6J9isn7VEEpWKvoiUXHoGtB3oT/E7/Hx/at/9bWDeI7qLn0gCUtEXkd1XdS/ocQ8Megdq7QfPD4UHO8Kyj0InE5EYKvoiUnrqH0OHXZoAAAzCSURBVAYXvga9R8HaH2DsiTD1CtiwOnQyEUFFX0RKW1oatD7b38Wv3RD4aBIMbwMfPgg7todOJ5LSVPRFJD4qZ0ZX9ZsJe7eAl/4AY0+AJR+ETiaSslT0RSS+6h4MF0yF08f7m/c82BGmDNGNfEQCUNGPA12RTyQXM2jRB4Z9CMdcAZ8+6Xv5zxkN27eFTieSMnRFvjjSFflE8rHyK3jlj/4GPnu3hG53QqN2oVNJgtAV+eJHLX0RKXt1DoTzpsAZD8OmX2D8KfDcYFj/c+hkIuWair6IhGEGh/T2N/I59ir4bDLcnw1zxuiQv0icqOiLSFiVqsHJN8GQ2ZB1mD/sP7YD/DAncDCR8kdFX0QSQ+2m0SH/Cf5iPuM7wZSh6uUvUopU9EUkcZjBIafG9PJ/Irqwzzhd2EekFKjoi0jiqVQNOt4Ml86CfVrBS1fDuJNg2dzQyUSSmoq+iCSuOs38hX36PAi//gRjT4KpV8LGX0InE0lKKvoiktjMoOXp/pB/u0vho4kwPFu37xUpARV9EUkOlWtA59vgkumwV1N/+96HOsPyz0InE0kaKvoiklz2aQEDXoFeD8DqRTC6Pbz6J9j8a+hkIglPRV9Ekk9aGhzWD4blQJsL4P2RMPwImP8M6NLiIvlS0ReR5LXHntD9brj4Lai+D0y+ECb1hlWLQicTSUgq+nGgu+yJlLEGbWDg29D1Tlj2EYw8Ct6+FbZuCp1MJKGo6MeBc26qc25QZmZm6CgiqSMtHdoO9If8DzkVpt8OD7SDr98InUwkYajoi0j5Un1vOG2MP78/vSI8ejo8eS6sWxo6mUhwKvoiUj7tdzwMngkn/tm39oe3hVn3w/atoZOJBKOiLyLlV0ZFOP4aGDoHGh8Lr9/oT/HTHfwkRanoi0j5V6sxnPMk9H0UNq/1d/B7fpgu5yspR0VfRFKDGRzcHYZ+AEdfDp88Dve38Zfz1bn9kiJU9EUktVSqBp3+7i/nW/vA6HK+XeHnhaGTicSdir6IpKa9D/GX8+15P6xcCKOOhTf+Cls2hE4mEjcq+iKSutLS4PDzYdhcaHUWzLwHRrSDL18NnUwkLlT0RUSq7gW9R/iWf8U94PG+8EQ/ndsv5Y6KvojITvseDZe8Byf9FRa9FZ3bPxy2bwudTKRUqOiLiMTKqAjH/QGGvg+Nj4HXb4AxHWBpTuhkIrtNRV9EJC+1GsM5T8GZk2Djahh3Mrx4FWxaGzqZSImp6IuI5McMmveEYR9Au0th7gQYfgR8+rTO7ZekpKIvIlKYStWh820waBpkNoBnL4ZJp8Lqb0InEykWFX0RkaKqdyhc/CZ0vROWzYUHjoJ3b4dtv4dOJlIkKvoiIsWRlg5tB/rL+R7UFd651V/Y57sZoZOJFEpFX0SkJGrUgzMmQL9nfEt/Qjd47lLYsDp0MpF8qejHgZn1MLMx69atCx1FROKt6ckw5H049ir47CkYnq2b+EjCUtGPA+fcVOfcoMzMzNBRRKQsVNwDTr7JX9hn5018JnSHlV+FTiayCxV9EZHSsndzfynfHvfCis9g5NHw9q2wdXPoZCKAir6ISOlKS4M2/WFYDhxyKky/3Rf/xdNCJxNR0RcRiYtqdaHPWDhvCrgdMLEXPHsJbFgVOpmkMBV9EZF4OuAEGDIbjrsG5j/jO/p9NEkd/SQIFX0RkXirUAVO+jMMngF1DoIXhvlT/FZ+GTqZpBgVfRGRslL3IOj/MvS4D1YsgJHHqKOflCkVfRGRspSWBm0uyKOj37uhk0kKUNEXEQmhWp2oo99zUUe/nuroJ3Gnoi8iEtIBJ0Yd/a6G+ZP9rXvnPaqOfhIXKvoiIqFVqAIn/cV39KvdFJ4fAg/3gFWLQieTckZFX0QkUdQ9GAa8Ct3vgZ8+hZFHwbR/6da9UmpU9EVEEklaGmQPgGEfwsE9YNo/olv3zgydTMoBFX0RkURUfW84fTz0mwzbNsOErvDCZbDxl9DJJImp6IuIJLKmHf2te4++3HfwG9EWPpusjn5SIir6IiKJrmJV6PR3GDQNMhvCMxfBI31gzXeBg0myUdEXEUkW9VrBxW9C53/Bkjkwoh3MvBe2bw2dTJKEir6ISDJJS4d2g2HoB9DkJHjjLzDmBFg6N3QySQIq+iIiySgzC856FPo+AhtXwbiT4OVr4fffQieTBKaiLyKSzA7uAUPnwBEXwwdjYMSR8MXLoVNJglLRFxFJdpUzodudcNEb/v9PnA1Pngu//hg6mSQYFX0RkfKi4RFwyXR/Sd+v3/Ct/g/Gwo4doZNJglDRFxEpT9Ir+Jv3XDoL6h8GL18D40+BFZ+HTiYJQEVfRKQ82usAOP956D0KVi+C0cfBWzfD1s2hk0lAKvoiIuWVGbQ+G4blQMsz4L27YOTR8O300MkkEBV9EZHyrupecOoo3/J3O/xte6cM0XX8U5CKvohIqti/AwyZDcdeBZ8+CcOPgE+f1nX8U4iKvohIKqlQBU6+CQa9C7Uaw7MX6zr+KURFX0QkFe3TAi56HbrcHnMd//tg+7bQySSOVPSLwcwONrNRZjbZzC4NnUdEZLekpcORl/gr+u3fAd74M4w9AX78OHQyiZOUKfpmNt7Mfjaz+bme72xmX5rZIjO7rqBxOOcWOucGA2cC2fHMKyJSZjIbwNmPw5kTYf0KX/hfuwG2bAidTEpZyhR9YALQOfYJM0sHRgBdgObA2WbW3MxamtmLuR51o/f0BGYAb5VtfBGRODKD5r383fsOvwBmD4cH2sHXb4ZOJqUoZYq+c246kPv8lLbAIufcYufcFuAJoJdz7jPnXPdcj5+j8bzgnDsa6Fe2n0BEpAxUqQk97oEBr0JGZXi0DzxzMaxfGTqZlIKUKfr5yAKWxPy9NHouT2bWwczuM7PRQJ63sTKzQWaWY2Y5K1dqIxGRJLXvUTB4BrS/DhZMgRFHwLxHdHpfkkv1om95PJfvGu2cm+acu9w5d4lzbkQ+w4xxzmU757Lr1KlTakFFRMpcRiU44U9w6UyocxA8P9Rf2Gf1N6GTSQmletFfCjSM+bsBoHtRiojEqtMM+r8M3e+Gnz7xl/J97y7YvjV0MimmVC/6HwJNzWw/M6sInAW8EDiTiEjiSUuD7At9R7+mnfzNe8Z0gGVzQyeTYkiZom9mjwOzgWZmttTMLnLObQOGAa8BC4GnnHMLQuYUEUloNepB30nQ91HYuBrGnQyvXAe/rw+dTIrAnDpllDoz6wH0aNKkycCvv/46dBwRkfjYvM63+D980J/r3+0uOPCU3R6tmc11zulaKHGQMi39suScm+qcG5SZmRk6iohI/FTO9IX+wtegYlV47Ex4egCs/zl0MsmHir6IiOyeRkfCJe/BCTfAFy/6u/d9/FjoVJIHFX0REdl9GRWh/bUweCbUbQ6rvgqdSPKQETqAiIiUI3UOhP4vwQ7drS8RqaUfB2bWw8zGrFu3LnQUEZGyl5bmW/6ScFT040Ad+UREJBGp6IuIiKQIFX0REZEUoaIvIiKSIlT0RUREUoSKvoiISIpQ0Y8DnbInIiKJSDfciSMzWwl8X8AgmUBB3wxqA6tKNVQ4hX3WZJru7o6zpO8vzvuKOmxRhitoGK2jiTvdEOtpcd+T3/D7OufqFHPaUhTOOT0CPYAxhbyeEzpjWX3WZJru7o6zpO8vzvuKOmxRhitoGK2jiTvdEOtpcd8Tap6n8kOH98OaGjpAGQr1WeMx3d0dZ0nfX5z3FXXYogyXKutpeVpHS2O8JXl/cd+TKutWwtDh/QRmZjlO95SWBKZ1VCS5qKWf2MaEDiBSCK2jIklELX0REZEUoZa+iIhIilDRFxERSREq+iIiIilCRT9JmFlVM3vYzMaaWb/QeUTyYmb7m9mDZjY5dBYR+V8q+gGZ2Xgz+9nM5ud6vrOZfWlmi8zsuujp04DJzrmBQM8yDyspqzjrqXNusXPuojBJRaQwKvphTQA6xz5hZunACKAL0Bw428yaAw2AJdFg28swo8gEir6eikgCU9EPyDk3Hfgl19NtgUVRi2kL8ATQC1iKL/yg5SZlqJjrqYgkMBWPxJPFf1v04It9FvAs0MfMRqJLV0p4ea6nZraXmY0CDjOzP4WJJiL5yQgdQP6H5fGcc85tAAaUdRiRfOS3nq4GBpd1GBEpGrX0E89SoGHM3w2AHwNlEcmP1lORJKSin3g+BJqa2X5mVhE4C3ghcCaR3LSeiiQhFf2AzOxxYDbQzMyWmtlFzrltwDDgNWAh8JRzbkHInJLatJ6KlB+64Y6IiEiKUEtfREQkRajoi4iIpAgVfRERkRShoi8iIpIiVPRFRERShIq+iIhIilDRFxERSREq+iIiIilCRV9ERCRF/D/fHXQO3h57nwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "####****KEPT IN PHASE 4 TO PROVIDE COMPARISON IN EVALUATION TEXT GENERATION IN PHASE 5..\n",
    "####****from phase 3 were we create new models of the language using the linear smoothing and weightings lambda\n",
    "####****the linear smoothing quadgram model has minor error in indexing and should be updated.\n",
    "#smoothing the ngramModel using a linear function of the kgrams for k = 1 to n\n",
    "def ngramModel_LinearSmooth(inlist, n):\n",
    "    #generate ngrams\n",
    "    total = len(inlist)\n",
    "    anoutcome = []\n",
    "    for i in range(1,n+1):\n",
    "        anoutcome.append(ngrams(inlist, i))\n",
    "        #print(\"outcome: \")\n",
    "        #print(anoutcome[i-1])\n",
    "        \n",
    "    #generate lamd coefficients for terms in model\n",
    "    k = 1\n",
    "    lamd = []\n",
    "    last_lamd = 0\n",
    "    for i in range(1,n):\n",
    "        lamd.append(random.uniform(0,k))\n",
    "        k = k-lamd[i -1]              \n",
    "    lamd.append(k)\n",
    "    print(\"lamd: \", lamd)\n",
    "    #generate smooth model\n",
    "    smooth_model = {}\n",
    "    sumoflinearsmoothunimodel = 0\n",
    "    for keyword in anoutcome[n-1]:\n",
    "        grams = keyword.split(' ')\n",
    "        #print(\"grams:\")\n",
    "        #print(grams)\n",
    "        smooth_model.setdefault(keyword, lamd[0]*anoutcome[0][grams[0]]/total)\n",
    "        for i in range(1,len(grams) - 2):\n",
    "            sub_string = ' '.join(grams[0:i])\n",
    "            sub_sub_string = ' '.join(input[0:i -1])\n",
    "           # print(sub_string)\n",
    "            smooth_model[keyword] = smooth_model[keyword] + lamd[i] * (anoutcome[i][sub_string]/anoutcome[i-1][keyword])\n",
    "        #print(keyword + \":\")\n",
    "        #print(smooth_model[keyword])\n",
    "        sumoflinearsmoothunimodel = sumoflinearsmoothunimodel + smooth_model[keyword]\n",
    "    #print(\"smooth_model:\")\n",
    "    #print(smooth_model)\n",
    "    print(f\"the sum of the linear smoothed unigram model is {sumoflinearsmoothunimodel}\")\n",
    "    return smooth_model\n",
    "\n",
    "linearsmoothunimodel = ngramModel_LinearSmooth(tokens, 1)\n",
    "\n",
    "pandas.set_option(\"display.max_rows\", 10)\n",
    "linearsmoothunidataframe = pandas.DataFrame.from_dict(linearsmoothunimodel, orient = 'index', columns = ['prob.'])\n",
    "print('Number of rows in Linear Smoothed Unigram Prob. Model : ', len(linearsmoothunidataframe.index))\n",
    "print(linearsmoothunidataframe)\n",
    "\n",
    "#Attempt to plot the unigram language model using first a Counter object\n",
    "COUNTLSMOOTH1 = Counter(linearsmoothunimodel)\n",
    "greatestlinearsmoothprob1 = 0\n",
    "biglinearsmoothword1 = ''\n",
    "for w in COUNTLSMOOTH1.keys():\n",
    "    if COUNTLSMOOTH1[w] >= greatestlinearsmoothprob1:\n",
    "        biglinearsmoothword1 = w\n",
    "        greatestlinearsmoothprob1 = COUNTLSMOOTH1[w]\n",
    "        \n",
    "print(f\"the unigram of greatest freq in the smoothed unigram model is: {biglinearsmoothword1} \\n\")\n",
    "MLS1 = COUNTLSMOOTH1[biglinearsmoothword1]\n",
    "yscale('log'); xscale('log'); title('Frequency of n-th most frequent 1-itemset in linear smoothed model and 1/n line.')\n",
    "\n",
    "plot([c for (w,c) in COUNTLSMOOTH1.most_common()])\n",
    "plot([(MLS1)/i for i in range(1, len(COUNTLSMOOTH1)+1)]);\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in Linear Smooth Unigram Prob. Model w/o test data:  63\n",
      "           prob.\n",
      "the     0.052632\n",
      "cat     0.021053\n",
      "named   0.010526\n",
      "bob     0.021053\n",
      "is      0.052632\n",
      "...          ...\n",
      "cats    0.010526\n",
      "should  0.010526\n",
      "not     0.010526\n",
      "exist   0.010526\n",
      "dogs    0.010526\n",
      "\n",
      "[63 rows x 1 columns]\n",
      "Number of rows in Linear Smooth Unigram Prob. Model w/ test data :  75\n",
      "              prob.\n",
      "the        0.052632\n",
      "cat        0.021053\n",
      "named      0.010526\n",
      "bob        0.021053\n",
      "is         0.052632\n",
      "...             ...\n",
      "sky        0.007692\n",
      "blue       0.007692\n",
      "today      0.007692\n",
      "shadow     0.007692\n",
      "yesterday  0.007692\n",
      "\n",
      "[75 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "### Now updating our linear smooth unigram model to include the test data...\n",
    "#####*******update the model so that tokens that are in the test text not in the training\n",
    "#####****** set have nonzero value...but something really small...use M = 1/(tot # in test + tot in training)\n",
    "traintestlinearsmoothunimodel = linearsmoothunimodel\n",
    "M = 0\n",
    "\n",
    "for w in testtokens:\n",
    "    M = M + 1\n",
    "    if w not in linearsmoothunimodel.keys():\n",
    "        traintestlinearsmoothunimodel.update({w: 0})       \n",
    "\n",
    "for w in linearsmoothunimodel.keys():\n",
    "    M = M + math.ceil(linearsmoothunimodel[w]*len(linearsmoothunimodel))\n",
    "\n",
    "for w in traintestlinearsmoothunimodel.keys():\n",
    "    if (traintestlinearsmoothunimodel[w] == 0):\n",
    "        traintestlinearsmoothunimodel[w] = 1/M \n",
    "        \n",
    "print('Number of rows in Linear Smooth Unigram Prob. Model w/o test data: ', len(linearsmoothunidataframe.index))        \n",
    "print(linearsmoothunidataframe) \n",
    "\n",
    "traintestlinearsmoothunidataframe = pandas.DataFrame.from_dict(traintestlinearsmoothunimodel, orient = 'index', columns = ['prob.'])\n",
    "print('Number of rows in Linear Smooth Unigram Prob. Model w/ test data : ', len(traintestlinearsmoothunidataframe.index))\n",
    "print(traintestlinearsmoothunidataframe)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For our Linear Smoothed unigrammodel of the language:\n",
      "\n",
      "The number of bits we would need to find the correct last word of given phrase with \n",
      "\n",
      "the Linear Smoothed unigram model is the entropgy H = 5.75039801431027\n",
      "The perplexity for the unigrammodel is 2^H = 53.83221989505292\n"
     ]
    }
   ],
   "source": [
    "#####***** entropy calculation then perplexity for our linear smoothed unigram model of the test data:\n",
    "####***** we will define our vocabulary to be all words in training + test set\n",
    "####***** Then the idea is that entropy for the model, or likelyhood that the\n",
    "####***** model would correctly predict the last word of a string of text in the form of test data\n",
    "####***** is related to -(the sum over all unigrams in vocubulary x_i of prob(x_i)*log(prob(x_i))\n",
    "TRAINTESTCOUNTLSMOOTH1 = Counter(traintestlinearsmoothunimodel)\n",
    "vocproblinear = [COUNTLSMOOTH1[w] for w in COUNTLSMOOTH1.keys()]\n",
    "\n",
    "entropysumlinear = 0\n",
    "for prob in vocproblinear:\n",
    "    entropysumlinear = entropysumlinear - (prob * math.log(prob, 2))\n",
    "print(\"For our Linear Smoothed unigrammodel of the language:\\n\")\n",
    "print(\"The number of bits we would need to find the correct last word of given phrase with \\n\")\n",
    "print(f\"the Linear Smoothed unigram model is the entropgy H = {entropysumlinear}\")\n",
    "perplexitylinear = (2**entropysumlinear)\n",
    "print(f\"The perplexity for the unigrammodel is 2^H = {perplexitylinear}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sum of all the unigram probabiities in the laplace smoothed model needs to be 1 and it is 0.9999999999999991\n",
      "Number of rows in Laplace Smoothed Unigram Prob. Model :  63\n",
      "           prob.\n",
      "the     0.028169\n",
      "cat     0.017606\n",
      "named   0.014085\n",
      "bob     0.017606\n",
      "is      0.028169\n",
      "...          ...\n",
      "cats    0.014085\n",
      "should  0.014085\n",
      "not     0.014085\n",
      "exist   0.014085\n",
      "dogs    0.014085\n",
      "\n",
      "[63 rows x 1 columns]\n",
      "the unigram of greatest freq in the Laplace smoothed unigram model is: is \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "###****From Phase 3 of the project the Laplace smoothed unigram model\n",
    "###****using the chosen training data test folder....relies on computation of the dataframes\n",
    "###****per file\n",
    "#Laplace smoothed unigram prob. model using prob(x) = (1 + frequency of x in corpus)/(total in corpus)\n",
    "\n",
    "#Also left in the event we want to take into consideration of the file size when smoothing\n",
    "#the language models... we created a Counter object for each file to seperate\n",
    "#the unigramsin each file and their frequency in the file...\n",
    "#the createListDoc_Foo_Counters below take in a list of strings, one fore each incoming file, which we\n",
    "#created when we read in the files ....the smoothing in the Laplace smoothing below do not weight\n",
    "#the files by size but do use these counters to tally up the total freqeuencies of ngrams and token count\n",
    "def createListDocUniCounter(inlist):\n",
    "    docfreqlist = []\n",
    "    for i in range(len(inlist)):\n",
    "        counter = Counter(newngram(inlist[i],1))\n",
    "        docfreqlist.append(counter)\n",
    "    return docfreqlist\n",
    "\n",
    "dfforuniperfile = createListDocUniCounter(tokenfilelist)\n",
    "firstunifile = dfforuniperfile[0]\n",
    "#print(dfforuniperfile)\n",
    "#print(firstunifile)\n",
    "\n",
    "def createLeplaceSmoothedUnigramModel(outset, dfperfilelist):\n",
    "    n = 1   \n",
    "    anoutcome = ngrams(outset,n)\n",
    "    sumoflaplaceprob = 0\n",
    "  \n",
    "    laplaceprobmodel = anoutcome\n",
    "    \n",
    "    for w in laplaceprobmodel:\n",
    "        laplaceprobmodel[w] = 0\n",
    "  \n",
    "    filecount = 0\n",
    "    for temp in anoutcome:\n",
    "        for i in range(len(dfperfilelist)):\n",
    "            count = dfperfilelist[i]\n",
    "            filecount = filecount + count[temp] + 1\n",
    "            \n",
    "    for keyword in anoutcome:\n",
    "        #print(keyword)\n",
    "        for i in range(len(dfperfilelist)):\n",
    "            count = dfperfilelist[i]\n",
    "            laplaceprobmodel[keyword] = laplaceprobmodel[keyword] + (count[keyword] + 1)/(filecount) \n",
    "        sumoflaplaceprob = sumoflaplaceprob + laplaceprobmodel[keyword]\n",
    "       \n",
    "        \n",
    "    #print(f\"The laplaceprobmodel is \\n {laplaceprobmodel}\")\n",
    "    print(f\"The sum of all the unigram probabiities in the laplace smoothed model needs to be 1 and it is {sumoflaplaceprob}\")\n",
    "    return laplaceprobmodel    \n",
    "\n",
    "laplacesmoothunimodel = createLeplaceSmoothedUnigramModel(tokens, dfforuniperfile)\n",
    "\n",
    "pandas.set_option(\"display.max_rows\", 10)\n",
    "laplacesmoothunidf = pandas.DataFrame.from_dict(laplacesmoothunimodel, orient = 'index', columns = ['prob.'])\n",
    "print('Number of rows in Laplace Smoothed Unigram Prob. Model : ', len(laplacesmoothunidf.index))\n",
    "print(laplacesmoothunidf)\n",
    "\n",
    "#Attempt to plot the unigram language model using first a Counter object\n",
    "COUNTLapSMOOTH1 = Counter(laplacesmoothunimodel)\n",
    "greatestlaplacesmoothprob1 = 0\n",
    "biglaplacesmoothword1 = ''\n",
    "for w in COUNTLapSMOOTH1.keys():\n",
    "    if COUNTLapSMOOTH1[w] >= greatestlaplacesmoothprob1:\n",
    "        biglaplacesmoothword1 = w\n",
    "        greatestlaplacesmoothprob1 = COUNTLapSMOOTH1[w]\n",
    "        \n",
    "print(f\"the unigram of greatest freq in the Laplace smoothed unigram model is: {biglaplacesmoothword1} \\n\")\n",
    "MLapS1 = COUNTLapSMOOTH1[biglaplacesmoothword1]\n",
    "yscale('log'); xscale('log'); title('Frequency of n-th most frequent 1-itemset in laplace smoothed model and 1/n line.')\n",
    "\n",
    "plot([c for (w,c) in COUNTLapSMOOTH1.most_common()])\n",
    "plot([(MLapS1)/i for i in range(1, len(COUNTLapSMOOTH1)+1)]);\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in Laplace Smooth Unigram Prob. Model w/o test data:  63\n",
      "           prob.\n",
      "the     0.028169\n",
      "cat     0.017606\n",
      "named   0.014085\n",
      "bob     0.017606\n",
      "is      0.028169\n",
      "...          ...\n",
      "cats    0.014085\n",
      "should  0.014085\n",
      "not     0.014085\n",
      "exist   0.014085\n",
      "dogs    0.014085\n",
      "\n",
      "[63 rows x 1 columns]\n",
      "Number of rows in Laplace Smooth Unigram Prob. Model w/ test data :  75\n",
      "              prob.\n",
      "the        0.028169\n",
      "cat        0.017606\n",
      "named      0.014085\n",
      "bob        0.017606\n",
      "is         0.028169\n",
      "...             ...\n",
      "sky        0.005988\n",
      "blue       0.005988\n",
      "today      0.005988\n",
      "shadow     0.005988\n",
      "yesterday  0.005988\n",
      "\n",
      "[75 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "### Now updating our laplace smooth unigram model to include the test data...\n",
    "#####*******update the model so that tokens that are in the test text not in the training\n",
    "#####****** set have nonzero value...but something really small...use M = 1/(tot # in test + tot in training)\n",
    "traintestlaplacesmoothunimodel = laplacesmoothunimodel\n",
    "M = 0\n",
    "\n",
    "for w in testtokens:\n",
    "    M = M + 1\n",
    "    if w not in laplacesmoothunimodel.keys():\n",
    "        traintestlaplacesmoothunimodel.update({w: 0})       \n",
    "\n",
    "for w in laplacesmoothunimodel.keys():\n",
    "    M = M + math.ceil(laplacesmoothunimodel[w]*len(laplacesmoothunimodel))\n",
    "\n",
    "for w in traintestlaplacesmoothunimodel.keys():\n",
    "    if (traintestlaplacesmoothunimodel[w] == 0):\n",
    "        traintestlaplacesmoothunimodel[w] = 1/M \n",
    "        \n",
    "print('Number of rows in Laplace Smooth Unigram Prob. Model w/o test data: ', len(laplacesmoothunidf.index))        \n",
    "print(laplacesmoothunidf) \n",
    "\n",
    "traintestlaplacesmoothunidataframe = pandas.DataFrame.from_dict(traintestlaplacesmoothunimodel, orient = 'index', columns = ['prob.'])\n",
    "print('Number of rows in Laplace Smooth Unigram Prob. Model w/ test data : ', len(traintestlaplacesmoothunidataframe.index))\n",
    "print(traintestlaplacesmoothunidataframe)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For our Laplace Smoothed unigrammodel of the language:\n",
      "\n",
      "The number of bits we would need to find the correct last word of given phrase with \n",
      "\n",
      "the Laplace Smoothed unigram model is the entropgy H = 6.478359226595207\n",
      "The perplexity for the laplace unigrammodel is 2^H = 89.16213309393059\n"
     ]
    }
   ],
   "source": [
    "#####***** entropy calculation then perplexity for our laplace smoothed unigram model of the test data:\n",
    "####***** we will define our vocabulary to be all words in training + test set\n",
    "####***** Then the idea is that entropy for the model, or likelyhood that the\n",
    "####***** model would correctly predict the last word of a string of text in the form of test data\n",
    "####***** is related to -(the sum over all unigrams in vocubulary x_i of prob(x_i)*log(prob(x_i))\n",
    "TRAINTESTCOUNTLAPSMOOTH1 = Counter(traintestlaplacesmoothunimodel)\n",
    "vocproblaplace = [TRAINTESTCOUNTLAPSMOOTH1[w] for w in TRAINTESTCOUNTLAPSMOOTH1.keys()]\n",
    "\n",
    "entropysumlaplace = 0\n",
    "for prob in vocproblaplace:\n",
    "    entropysumlaplace = entropysumlaplace - (prob * math.log(prob, 2))\n",
    "print(\"For our Laplace Smoothed unigrammodel of the language:\\n\")\n",
    "print(\"The number of bits we would need to find the correct last word of given phrase with \\n\")\n",
    "print(f\"the Laplace Smoothed unigram model is the entropgy H = {entropysumlaplace}\")\n",
    "perplexitylaplace = (2**entropysumlaplace)\n",
    "print(f\"The perplexity for the laplace unigrammodel is 2^H = {perplexitylaplace}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### in an extension of the evaluation of the models, we could take the test data\n",
    "####  break it into ngrams and peal off the last word.\n",
    "####  for each of the ngrams we test and peal off the last word...generate a ranked set of next words, i.e.\n",
    "####. our text generation method could be scaled to return not just one possible next word...but a ranked\n",
    "####  list of next words...\n",
    "####. this list of next words is a list of the predicted last word of the ngram from the test data...\n",
    "####. how well did we do?  Is the correct last word of the actual ngram from the test data in the top 10%\n",
    "####. of our returned list?  We have quantified that process above...but we leave the text generation \n",
    "####. modules here for that chance...\n",
    "ranint = random.randint(0,len(laplacesmoothunimodel)-1)\n",
    "print(ranint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####***** if you are feeling like generating a random seed for the text:\n",
    "i = 0;\n",
    "lapcounter = Counter(laplacesmoothunimodel)\n",
    "ranint = random.randint(0, len(laplacesmoothunimodel)-1)\n",
    "for w in laplacesmoothunimodel.keys():\n",
    "    if (i == ranint):\n",
    "        seedword = w\n",
    "    i = i + 1\n",
    "print(seedword)\n",
    "\n",
    "####**** to set the seed to one of the most common 10 unigrams:\n",
    "seedpossibilities = lapcounter.most_common(10)\n",
    "ranint = random.randint(0,9)\n",
    "seedtuple = seedpossibilities[ranint]\n",
    "seedword = seedtuple[0]\n",
    "print(seedword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### from phase 2, The team kept both ngrams method and newngram method for computing the \n",
    "###unigrams, bigrams, trigrams and quadgrams smoothed models....\n",
    "###output of newngram is a Counter obj and output of ngrams is a dictionary object...\n",
    "\n",
    "#newngram outputs to files:\n",
    "#the most common unigrams are set to unigramfile.dat\n",
    "#the most common bigrams are set to bigramfile.dat\n",
    "#the most common trigrams are set to trigramfile.dat\n",
    "#the most common quadgrams are set to quadgramfile.dat\n",
    "\n",
    "#!!!newngram again returns a Counter object \n",
    "def newngram(toks, n):\n",
    "    output = {}   \n",
    "    for i in range(len(toks) - n + 1):\n",
    "        g = ' '.join(toks[i:i+n])\n",
    "        output.setdefault(g, 0)\n",
    "        output[g] += 1\n",
    "    COUNTS = Counter(output)\n",
    "    outputstring = ''\n",
    "    outputstring = outputstring + str(COUNTS.most_common(3000)) + \" \"\n",
    "    if n == 1:\n",
    "        #print(f\"\\n   The most common unigrams are: {(COUNTS.most_common(10))}\")\n",
    "        f=open(\"unigramfile.dat\",\"w+\", encoding='utf-8', errors='replace')\n",
    "        f.write(str(sum(COUNTS.values())))\n",
    "        f.write(str(COUNTS.most_common(3000))) #trying to keep file size at about 50 k for this sample\n",
    "        outputstring = outputstring + str(COUNTS.most_common(3000)) + \" \"\n",
    "        f.close()\n",
    "    if n == 2:\n",
    "        #print(f\"\\n   The most common bigrams are: {(COUNTS.most_common(10))}\")\n",
    "        f=open(\"bigramfile.dat\",\"w+\", encoding='utf-8', errors='replace')\n",
    "        f.write(str(sum(COUNTS.values())))\n",
    "        f.write(str(COUNTS.most_common(2700))) #trying to keep file size at about 50 k for this sample\n",
    "        outputstring = outputstring + str(COUNTS.most_common(2700)) + \" \"\n",
    "        f.close()\n",
    "    if n == 3:\n",
    "        #print(f\"\\n The most common trigrams are: {(COUNTS.most_common(10))}\")\n",
    "        f=open(\"trigramfile.dat\",\"w+\", encoding='utf-8', errors='replace')\n",
    "        f.write(str(sum(COUNTS.values())))\n",
    "        f.write(str(COUNTS.most_common(2300))) #trying to keep file size at about 50 k for this sample\n",
    "        outputstring = outputstring + str(COUNTS.most_common(2300)) + \" \"\n",
    "        f.close()\n",
    "    if n == 4:\n",
    "        #print(f\"\\n   The most common quadgrams are: {(COUNTS.most_common(10))}\")\n",
    "        f=open(\"quadgramfile.dat\",\"w+\", encoding='utf-8', errors='replace')\n",
    "        f.write(str(sum(COUNTS.values())))\n",
    "        f.write(str(COUNTS.most_common(2100))) #trying to keep file size at about 50 k for this sample\n",
    "        outputstring = outputstring + str(COUNTS.most_common(2100)) + \" \"\n",
    "        f.close()\n",
    "    \n",
    "    return output\n",
    "\n",
    "###!!!! THESE COUNTS WILL BE USED IN THE TEXT GENERATION METHOD BELOW...THE PHASE 3 LAPLACE SMOOTH\n",
    "###!!! MODELS ARE CREATED IMPLICITLY WITHIN THE GENERATING TEXT MODULE\n",
    "newunigrams = newngram(tokens, 1)\n",
    "print(unigrams)\n",
    "newbigrams = newngram(tokens, 2)\n",
    "#print(bigrams)\n",
    "newtrigrams = newngram(tokens, 3)\n",
    "#print(trigrams)\n",
    "newquadgrams = newngram(tokens, 4)\n",
    "#print(quadgrams)\n",
    "\n",
    "####piecing together the development from when we posted the most common unigrams, bigrams\n",
    "####trigrams and quadgrams to the files..now we are able to use them in this application \n",
    "####to generate text....\n",
    "\n",
    "unigramfile=\"unigramfile.dat\"\n",
    "bigramfile=\"bigramfile.dat\"\n",
    "trigramfile=\"trigramfile.dat\"\n",
    "quadgramfile=\"quadgramfile.dat\"\n",
    "\n",
    "with open(unigramfile, 'rb', 0) as file, \\\n",
    "    mmap.mmap(file.fileno(), 0, access=mmap.ACCESS_READ) as text:\n",
    "    unigramtotal= text.read(text.find(b'[')).decode('utf-8')\n",
    "    unigrams= text.read(text.find(b']')).decode('utf-8')\n",
    "\n",
    "with open(bigramfile, 'rb', 0) as file, \\\n",
    "    mmap.mmap(file.fileno(), 0, access=mmap.ACCESS_READ) as text:\n",
    "    bigramtotal= text.read(text.find(b'[')).decode('utf-8')\n",
    "    bigrams= text.read(text.find(b']')).decode('utf-8')\n",
    "\n",
    "with open(trigramfile, 'rb', 0) as file, \\\n",
    "    mmap.mmap(file.fileno(), 0, access=mmap.ACCESS_READ) as text:\n",
    "    trigramtotal= text.read(text.find(b'[')).decode('utf-8')\n",
    "    trigrams= text.read(text.find(b']')).decode('utf-8')\n",
    "    \n",
    "with open(quadgramfile, 'rb', 0) as file, \\\n",
    "    mmap.mmap(file.fileno(), 0, access=mmap.ACCESS_READ) as text:\n",
    "    quadgramtotal= text.read(text.find(b'[')).decode('utf-8')\n",
    "    quadgrams= text.read(text.find(b']')).decode('utf-8')\n",
    "\n",
    "words=unigrams.replace('[','').replace(']','').replace('(','').replace(')','').split(',')\n",
    "unigramrange=0\n",
    "for w in range(1,len(words)-1,2):\n",
    "    unigramrange +=int(words[w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####****Finally Phase 4! Here is the geneartion method.  In the cell above,\n",
    "####****you created a seedword for generating the text...either from the most frequent 10\n",
    "####****unigramsstring or a random unigram...\n",
    "\n",
    "def generate(seedtext, length):\n",
    "    \n",
    "    if length==0:\n",
    "        output=''\n",
    "    print(\"Scroll down for the final result.  Here is the process used:\\n\")\n",
    "    for gword in range(1,length+1):\n",
    "        if gword > 3:                     # use quadgram model\n",
    "\n",
    "            \n",
    "            print(f\"Searching quadigrams for '{currenttrigram}'.\")             \n",
    "            quadgramoccurance = []\n",
    "            quadgramoccurance.append(quadgrams.find(\"'\" + currenttrigram + ' '))\n",
    "            if quadgramoccurance[0] > -1:\n",
    "                possiblequadgram = []\n",
    "                possiblequadfrequency = []\n",
    "                possiblequadtotalfreq = 0\n",
    "                n = 0\n",
    "                possiblequadgram.append(quadgrams[quadgramoccurance[n]+len(currenttrigram)+2:quadgrams.find(\"'\",quadgramoccurance[n]+len(currenttrigram)+2)])\n",
    "                try:\n",
    "                    possiblequadfrequency.append(int(quadgrams[quadgrams.find(\"', \",quadgramoccurance[n])+3:quadgrams.find(\")\",quadgramoccurance[n])]))\n",
    "                    possiblequadtotalfreq += possiblequadfrequency[n]\n",
    "                except:\n",
    "                    print(\"Error\")\n",
    "                n += 1\n",
    "                while True:\n",
    "                    quadgramoccurance.append(quadgrams.find(\"'\" + currenttrigram + ' ', quadgramoccurance[n-1]+1))\n",
    "    \n",
    "                    if quadgramoccurance[n] == -1: break\n",
    "                    possiblequadgram.append(quadgrams[quadgramoccurance[n]+len(currenttrigram)+2:quadgrams.find(\"'\",quadgramoccurance[n]+len(currenttrigram)+2)])\n",
    "                    try:\n",
    "                        possiblequadfrequency.append(int(quadgrams[quadgrams.find(\"', \",quadgramoccurance[n])+3:quadgrams.find(\")\",quadgramoccurance[n])]))\n",
    "                        possiblequadtotalfreq += possiblequadfrequency[n]\n",
    "                    except:\n",
    "                        print(\"Error\")\n",
    "                        break\n",
    "                    n += 1\n",
    "                rand=random.randint(0,possiblequadtotalfreq)\n",
    "                look = rand\n",
    "                for w in range(0,n):\n",
    "                    look = look - possiblequadfrequency[w]\n",
    "                    if look < 0:\n",
    "                        nextword = possiblequadgram[w]\n",
    "                        break\n",
    "                print(f\"        Out of {possiblequadtotalfreq} occurances in the quadgram model the following word:\")\n",
    "                for w in range(0,n):\n",
    "                    print(f\"          '{possiblequadgram[w]}' appeared {possiblequadfrequency[w]} times,\")\n",
    "                print(f\"            From the {n} possibilities, we randomly chose '{nextword}'.\")                             \n",
    "            else:\n",
    "                print(f\"    Not found. Searching trigrams for '{currentbigram}'.\")  \n",
    "                trigramoccurance = []\n",
    "                trigramoccurance.append(trigrams.find(\"'\" + currentbigram + ' '))\n",
    "                if trigramoccurance[0] > -1:\n",
    "                    possibletrigram = []\n",
    "                    possibletrifrequency = []\n",
    "                    possibletritotalfreq = 0\n",
    "                    n = 0\n",
    "                    possibletrigram.append(trigrams[trigramoccurance[n]+len(currentbigram)+2:trigrams.find(\"'\",trigramoccurance[n]+len(currentbigram)+2)])\n",
    "                    try:\n",
    "                        possibletrifrequency.append(int(trigrams[trigrams.find(\"', \",trigramoccurance[n])+3:trigrams.find(\")\",trigramoccurance[n])]))\n",
    "                        possibletritotalfreq += possibletrifrequency[n]\n",
    "                    except:\n",
    "                        print(\"Error\")\n",
    "                    n += 1\n",
    "                    while True:\n",
    "                        trigramoccurance.append(trigrams.find(\"'\" + currentbigram + ' ', trigramoccurance[n-1]+1))\n",
    "                        if trigramoccurance[n] == -1: break\n",
    "                        possibletrigram.append(trigrams[trigramoccurance[n]+len(currentbigram)+2:trigrams.find(\"'\",trigramoccurance[n]+len(currentbigram)+2)])\n",
    "                        try:\n",
    "                            possibletrifrequency.append(int(trigrams[trigrams.find(\"', \",trigramoccurance[n])+3:trigrams.find(\")\",trigramoccurance[n])]))\n",
    "                            possibletritotalfreq += possibletrifrequency[n]\n",
    "                        except:\n",
    "                            print(\"Error\")\n",
    "                            break\n",
    "                        n += 1\n",
    "                    rand=random.randint(0,possibletritotalfreq)\n",
    "                    look = rand\n",
    "                    for w in range(0,n):\n",
    "                        look = look - possibletrifrequency[w]\n",
    "                        if look < 0:\n",
    "                            nextword = possibletrigram[w]\n",
    "                            break\n",
    "                    print(f\"        Out of {possibletritotalfreq} occurances in the trigram model the following word:\")\n",
    "                    for w in range(0,n):\n",
    "                        print(f\"          '{possibletrigram[w]}' appeared {possibletrifrequency[w]} times,\")\n",
    "                    print(f\"            From the {n} possibilities, we randomly chose '{nextword}'.\")                             \n",
    "                else:\n",
    "                    print(f\"        Not found. Searching bigrams for '{currentword}'.\")        \n",
    "                    bigramoccurance = []\n",
    "                    bigramoccurance.append(bigrams.find(\"'\" + currentword + ' '))\n",
    "                    if bigramoccurance[0] > -1:\n",
    "                        possiblebigram = []\n",
    "                        possiblebifrequency = []\n",
    "                        possiblebitotalfreq = 0\n",
    "                        n = 0\n",
    "                        possiblebigram.append(bigrams[bigramoccurance[n]+len(currentword)+2:bigrams.find(\"'\",bigramoccurance[n]+len(currentword)+2)])\n",
    "                        try:\n",
    "                            possiblebifrequency.append(int(bigrams[bigrams.find(\"', \",bigramoccurance[n])+3:bigrams.find(\")\",bigramoccurance[n])]))\n",
    "                            possiblebitotalfreq += possiblebifrequency[n]\n",
    "                        except:\n",
    "                            print(\"Error\")\n",
    "                        n += 1\n",
    "                        while True:\n",
    "                            bigramoccurance.append(bigrams.find(\"'\" + currentword + ' ', bigramoccurance[n-1]+1))\n",
    "                            nextword = bigrams[bigrams.find(\"'\" + currentword + ' ')+len(currentword)+2:bigrams.find(\"'\",bigrams.find(currentword + ' '))]\n",
    "                            if bigramoccurance[n] == -1: break\n",
    "                            possiblebigram.append(bigrams[bigramoccurance[n]+len(currentword)+2:bigrams.find(\"'\",bigramoccurance[n]+len(currentword)+2)])\n",
    "                            try:\n",
    "                                possiblebifrequency.append(int(bigrams[bigrams.find(\"', \",bigramoccurance[n])+3:bigrams.find(\")\",bigramoccurance[n])]))\n",
    "                                possiblebitotalfreq += possiblebifrequency[n]\n",
    "                            except:\n",
    "                                print(\"Error\")\n",
    "                                break\n",
    "                            n += 1\n",
    "                        rand=random.randint(0,possiblebitotalfreq)\n",
    "                        look = rand\n",
    "                        for w in range(0,n):\n",
    "                            look = look - possiblebifrequency[w]\n",
    "                            if look < 0:\n",
    "                                nextword = possiblebigram[w]\n",
    "                                break\n",
    "                        print(f\"          Out of {possiblebitotalfreq} occurances in the bigram model the following word:\")\n",
    "                        for w in range(0,n):\n",
    "                            print(f\"            '{possiblebigram[w]}' appeared {possiblebifrequency[w]} times,\")\n",
    "                        print(f\"              From the {n} possibilities, we randomly chose '{nextword}'.\")                \n",
    "                    else:\n",
    "                        rand=random.randint(0,unigramrange)\n",
    "                        look = rand\n",
    "                        for w in range(1,len(words)-1,2):\n",
    "                            look = look - int(words[w])\n",
    "                            if look < 0:\n",
    "                                nextword = words[w-1][2:-1]\n",
    "                                break\n",
    "                        print(f\"            Not found. We randomly choose '{nextword}'.\")\n",
    "            pastword =currentword\n",
    "            currentword=nextword\n",
    "            currentquadgram=currenttrigram + ' ' + currentword\n",
    "            currenttrigram=currentbigram + ' ' + currentword\n",
    "            currentbigram=pastword + ' ' + currentword\n",
    "            output+=' ' + currentword \n",
    "            \n",
    "            \n",
    "        else:\n",
    "            if gword == 3:                 # use trigram model\n",
    "                print(f\"      Searching trigrams for '{currentbigram}'.\")        \n",
    "                trigramoccurance = []\n",
    "                trigramoccurance.append(trigrams.find(\"'\" + currentbigram + ' '))\n",
    "                if trigramoccurance[0] > -1:\n",
    "                    possibletrigram = []\n",
    "                    possibletrifrequency = []\n",
    "                    possibletritotalfreq = 0\n",
    "                    n = 0\n",
    "                    possibletrigram.append(trigrams[trigramoccurance[n]+len(currentbigram)+2:trigrams.find(\"'\",trigramoccurance[n]+len(currentbigram)+2)])\n",
    "                    try:\n",
    "                        possibletrifrequency.append(int(trigrams[trigrams.find(\"', \",trigramoccurance[n])+3:trigrams.find(\")\",trigramoccurance[n])]))\n",
    "                        possibletritotalfreq += possibletrifrequency[n]\n",
    "                    except:\n",
    "                        print(\"Error\")\n",
    "                    n += 1\n",
    "                    while True:\n",
    "                        trigramoccurance.append(trigrams.find(\"'\" + currentbigram + ' ', trigramoccurance[n-1]+1))\n",
    "                        if trigramoccurance[n] == -1: break\n",
    "                        possibletrigram.append(trigrams[trigramoccurance[n]+len(currentbigram)+2:trigrams.find(\"'\",trigramoccurance[n]+len(currentbigram)+2)])\n",
    "                        try:\n",
    "                            possibletrifrequency.append(int(trigrams[trigrams.find(\"', \",trigramoccurance[n])+3:trigrams.find(\")\",trigramoccurance[n])]))\n",
    "                            possibletritotalfreq += possibletrifrequency[n]\n",
    "                        except:\n",
    "                            print(\"Error\")\n",
    "                            break\n",
    "                        n += 1\n",
    "                    rand=random.randint(0,possibletritotalfreq)\n",
    "                    look = rand\n",
    "                    for w in range(0,n):\n",
    "                        look = look - possibletrifrequency[w]\n",
    "                        if look < 0:\n",
    "                            nextword = possibletrigram[w]\n",
    "                            break\n",
    "                    print(f\"        Out of {possibletritotalfreq} occurances in the trigram model the following word:\")\n",
    "                    for w in range(0,n):\n",
    "                        print(f\"          '{possibletrigram[w]}' appeared {possibletrifrequency[w]} times,\")\n",
    "                    print(f\"            From the {n} possibilities, we randomly chose '{nextword}'.\")                             \n",
    "                else:\n",
    "                    print(f\"        Not found. Searching bigrams for '{currentword}'.\")        \n",
    "                    bigramoccurance = []\n",
    "                    bigramoccurance.append(bigrams.find(\"'\" + currentword + ' '))\n",
    "                    if bigramoccurance[0] > -1:\n",
    "                        possiblebigram = []\n",
    "                        possiblebifrequency = []\n",
    "                        possiblebitotalfreq = 0\n",
    "                        n = 0\n",
    "                        possiblebigram.append(bigrams[bigramoccurance[n]+len(currentword)+2:bigrams.find(\"'\",bigramoccurance[n]+len(currentword)+2)])\n",
    "                        try:\n",
    "                            possiblebifrequency.append(int(bigrams[bigrams.find(\"', \",bigramoccurance[n])+3:bigrams.find(\")\",bigramoccurance[n])]))\n",
    "                            possiblebitotalfreq += possiblebifrequency[n]\n",
    "                        except:\n",
    "                            print(\"Error\")\n",
    "                        n += 1\n",
    "                        while True:\n",
    "                            bigramoccurance.append(bigrams.find(\"'\" + currentword + ' ', bigramoccurance[n-1]+1))\n",
    "    \n",
    "                            if bigramoccurance[n] == -1: break\n",
    "                            possiblebigram.append(bigrams[bigramoccurance[n]+len(currentword)+2:bigrams.find(\"'\",bigramoccurance[n]+len(currentword)+2)])\n",
    "                            try:\n",
    "                                possiblebifrequency.append(int(bigrams[bigrams.find(\"', \",bigramoccurance[n])+3:bigrams.find(\")\",bigramoccurance[n])]))\n",
    "                                possiblebitotalfreq += possiblebifrequency[n]\n",
    "                            except:\n",
    "                                print(\"Error\")\n",
    "                                break\n",
    "                            n += 1\n",
    "                        rand=random.randint(0,possiblebitotalfreq)\n",
    "                        look = rand\n",
    "                        for w in range(0,n):\n",
    "                            look = look - possiblebifrequency[w]\n",
    "                            if look < 0:\n",
    "                                nextword = possiblebigram[w]\n",
    "                                break\n",
    "                        print(f\"          Out of {possiblebitotalfreq} occurances in the bigram model the following word:\")\n",
    "                        for w in range(0,n):\n",
    "                            print(f\"            '{possiblebigram[w]}' appeared {possiblebifrequency[w]} times,\")\n",
    "                        print(f\"              From the {n} possibilities, we randomly chose '{nextword}'.\")                \n",
    "                    else:\n",
    "                        rand=random.randint(0,unigramrange)\n",
    "                        look = rand\n",
    "                        for w in range(1,len(words)-1,2):\n",
    "                            look = look - int(words[w])\n",
    "                            if look < 0:\n",
    "                                nextword = words[w-1][2:-1]\n",
    "                                break\n",
    "                        print(f\"            Not found. We randomly choose '{nextword}'.\")\n",
    "                pastword =currentword\n",
    "                currentword=nextword\n",
    "                currenttrigram=currentbigram + ' ' + currentword\n",
    "                currentbigram=pastword + ' ' + currentword\n",
    "                output+=' ' + currentword \n",
    "            elif gword == 2:                 # use bigram model\n",
    "                print(f\"        Searching bigrams for '{currentword}'.\")        \n",
    "                bigramoccurance = []\n",
    "                bigramoccurance.append(bigrams.find(\"'\" + currentword + ' '))\n",
    "                if bigramoccurance[0] > -1:\n",
    "                    possiblebigram = []\n",
    "                    possiblebifrequency = []\n",
    "                    possiblebitotalfreq = 0\n",
    "                    n = 0\n",
    "                    possiblebigram.append(bigrams[bigramoccurance[n]+len(currentword)+2:bigrams.find(\"'\",bigramoccurance[n]+len(currentword)+2)])\n",
    "                    try:\n",
    "                        possiblebifrequency.append(int(bigrams[bigrams.find(\"', \",bigramoccurance[n])+3:bigrams.find(\")\",bigramoccurance[n])]))\n",
    "                        possiblebitotalfreq += possiblebifrequency[n]\n",
    "                    except:\n",
    "                        print(\"Error\")\n",
    "                    n += 1\n",
    "                    while True:\n",
    "                        bigramoccurance.append(bigrams.find(\"'\" + currentword + ' ', bigramoccurance[n-1]+1))\n",
    "                        if bigramoccurance[n] == -1: break\n",
    "                        possiblebigram.append(bigrams[bigramoccurance[n]+len(currentword)+2:bigrams.find(\"'\",bigramoccurance[n]+len(currentword)+2)])\n",
    "                        try:\n",
    "                            possiblebifrequency.append(int(bigrams[bigrams.find(\"', \",bigramoccurance[n])+3:bigrams.find(\")\",bigramoccurance[n])]))\n",
    "                            possiblebitotalfreq += possiblebifrequency[n]\n",
    "                        except:\n",
    "                            print(\"Error\")\n",
    "                            break\n",
    "                        n += 1\n",
    "                    rand=random.randint(0,possiblebitotalfreq)\n",
    "                    look = rand\n",
    "                    for w in range(0,n):\n",
    "                        look = look - possiblebifrequency[w]\n",
    "                        if look < 0:\n",
    "                            nextword = possiblebigram[w]\n",
    "                            break\n",
    "                    print(f\"          Out of {possiblebitotalfreq} occurances in the bigram model the following word:\")\n",
    "                    for w in range(0,n):\n",
    "                        print(f\"            '{possiblebigram[w]}' appeared {possiblebifrequency[w]} times,\")\n",
    "                    print(f\"              From the {n} possibilities, we randomly chose '{nextword}'.\")                \n",
    "                else:\n",
    "                    rand=random.randint(0,unigramrange)\n",
    "                    look = rand\n",
    "                    for w in range(1,len(words)-1,2):\n",
    "                        look = look - int(words[w])\n",
    "                        if look < 0:\n",
    "                            nextword = words[w-1][2:-1]\n",
    "                            break\n",
    "                    print(f\"            Not found. We randomly choose '{nextword}'.\")\n",
    "                pastword =currentword\n",
    "                currentword=nextword\n",
    "                currentbigram=pastword + ' ' + currentword\n",
    "                output+=' ' + currentword\n",
    "\n",
    "            elif gword == 1:                     # check seedtext\n",
    "                for char in range(0,len(seedtext)):\n",
    "                    maybe = seedtext[:len(seedtext)-char]\n",
    "                    print(f\"          Searching unigrams for '{maybe}'.\")        \n",
    "                    if unigrams.find(maybe) > -1:\n",
    "                        if unigrams[unigrams.find(maybe)-1] == \"'\":\n",
    "                            print(\"            Found the word (or a word that starts with it).\")\n",
    "                            currentword = unigrams[unigrams.find(maybe):unigrams.find(\"'\",unigrams.find(maybe))]\n",
    "                            break\n",
    "                    print(\"            Not found. Dropping a letter\")\n",
    "                    currentword = \"\"           \n",
    "                if currentword == \"\":\n",
    "                    rand=random.randint(0,unigramrange)\n",
    "                    look = rand\n",
    "                    for w in range(1,len(words)-1,2):\n",
    "                        look = look - int(words[w])\n",
    "                        if look < 0:\n",
    "                            currentword = words[w-1][2:-1]    \n",
    "                            break\n",
    "                    print(f\"              We randomly choose '{currentword}'.\")\n",
    "                output=currentword\n",
    "    \n",
    "    print(f\"\\n\\n     Given '{seedtext}', our initial model generates the following {length} words:\\n\\n{output.replace(' .','.')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate(seedword, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
